[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Projects",
    "section": "",
    "text": "Recent Certifications\n\nEsri Massive Open Online Courses\n\nGIS for Climate Action view certificate\nMake an Impact with Modern Geo Apps view certificate\n\n\n\n\nCapstone Projects\n\nUnderstanding the Influence of Parameter Value Uncertainty on Climate Model Outputs\nClimate models are computer simulations that attempt to replicate the complex interactions between Earth’s systems. Improving the accuracy of climate models relies on evaluating uncertainty and minimizing error. The Climate and Global Dynamics Lab at the National Center for Atmospheric Research (NCAR) has recently carried out a Parameter Perturbation Experiment (PPE) to understand how the uncertainty of parameter values affected the output of their model, the Community Land Model (CLM); which simulates terrestrial processes. While the necessary data for the PPE has been collected, the data is stored in a collection of files that are difficult to interpret in their current form. The current website hosts visualizations for a portion of the PPE data, but contains no visualizations for data that more closely simulates Earth system interactions. These issues can be mitigated by developing an emulator with the internal complexity to isolate a one-to-one relationship between a parameter and climate variable, then display the predicted relationship. A publicly available emulator with these capabilities will allow scientists to easily interpret complex climate model outputs and offer insights on parameter-variable relationships that are not being predicted accurately by the model; which can lead to increased accuracy and precision of climate models.\n\n\nDevelopment of a Solid State Fermentation Reactor for High-Value Protein-Enriched Feed from Almond Hulls\nEach year 4.5 billion pounds of almond hulls are produced in California as a byproduct of the almond industry. Currently almond hulls are being used as feed for the dairy industry, but there is a need to find alternative markets. From previous research, it was discovered that the protein content of almond hulls could be increased from 4-6% up to 15% for use as poultry feed through solid state fermentation. The goal of this project is to scale up the fermentation process and design a bioreactor for fermenting almond hulls using M. Thermophila under controlled conditions. The bioreactor holds roughly 100 grams of almond hull substrate per batch and is kept at 50°C with a relative humidity of 70-90%. After 96 hours of fermentation and post processing, a feed product with an enriched protein content of up to 15% is produced.\n\n\n\nRelevant Coursework\nCheck out some of the projects I’ve completed using R, Python, and SQL as part of my Master of Environmental Data Science\n\nGeospatial Analysis & Remote Sensing\nThe goal of this course was to introduce spatial modeling and analytic techniques of geographic information science to the Master of Environmental Data Science students. This course focused on creating open-source, reproducible workflows for geospatial problem solving in environmental sciences as well as working with different types of remotely sensed data. With an added focus on incorporating environmental justice into our project scope, we routinely worked with U.S. Census tract data to practice considering the human element behind data. Some of the key topics for this course included:\n\nUnderstanding Geospatial Datatypes: Vector vs Raster Data Manipulation\nHow we collect remotely sensed data: Active vs Passive Remote Sensing\n\n\nCheck out my work:\n\n\n\n\n\n\n\n\n\n\nExploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas\n\n\nGeospatial Analysis & Remote Sensing Course Project\n\n\n\nHeather Childers\n\n\nDec 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying Suitable Growth Area for Aquaculture Species Along the West Coast\n\n\nGeospatial Analysis & Remote Sensing Course Project\n\n\n\nHeather Childers\n\n\nDec 9, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nData Visualization\n\n\nMachine Learning\n\n\nPython for Environmental Data Science\n\n\nEnvironmental Policy Analysis for Data Science"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-9/index.html",
    "href": "Geospatial_Blogs/2023-12-9/index.html",
    "title": "Quantifying Suitable Growth Area for Aquaculture Species Along the West Coast",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production. Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, bottom depth .\nThis exercise uses suitable temperature and depth data for each Exclusive Economic Zones (EEZ) on the West Coast of the US to find the area that are best suited to developing marine aquaculture for several species of oysters.\nThis workflow is then expanded to create a function that can take in temperature and depth limits and produce graphs that show the area in square kilometers and the percentage of each EEZ that is suitable for that species’ suitable conditions.\nThis repository also contains an R script with the finalized function inside.\n\n\n\nCombining vector/raster data\nRe-sampling raster data\nMasking raster data\nMap algebra\n\n\n\n\n\nTo download the data used for this project, check out the link in my GitHub Repository!\n\n\nWe will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\n\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org.\n\n\n\nTo characterize the depth of the ocean we will use the General Bathymetric Chart of the Oceans (GEBCO).1\n\n\n\n\n\n\n\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(tmap)\nlibrary(cowplot)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(devtools)\nlibrary(tmaptools)\nlibrary(maptiles)\n\n\n\n\n\n\nRead in the SST Data using the terra::rast() function. Make sure you are in the correct working directory when reading in this data. If you are having issues, you can download my Rmarkdown file from my GitHub repository which uses here() to make this workflow more reproducible, assuming you followed the same folder structure outlined in the repository README.\n\n#Read in each of the Annual SSTs from 2008-2012\nsst_2008 &lt;- rast('data/average_annual_sst_2008.tif')\nsst_2009 &lt;- rast('data/average_annual_sst_2009.tif')\nsst_2010 &lt;- rast('data/average_annual_sst_2010.tif')\nsst_2011 &lt;- rast('data/average_annual_sst_2011.tif')\nsst_2012 &lt;- rast('data/average_annual_sst_2012.tif')\n\n#Create a rasteer stack from the 5 individual rasters\nsst &lt;- c(sst_2008,\n         sst_2009,\n         sst_2010,\n         sst_2011,\n         sst_2012)\n\n#Plot the raster stack to make sure the data was read in and stacked properly\nplot(sst)\n\n\n\n\n\n\n\nRead in the EEZ data using the sf::st_read() function.\nIt’s important that when we are trying to work with different datatsets, we make sure that they have the same CRS. Take this opportunity to re-project the EEZ data to the same CRS as the SST data so it’s easier to manipulate throughout the workflow.\n\ncoast &lt;- st_read('data/wc_regions_clean.shp')\n\n#Reproject the coastal data to the same CRS as the SST Data\ncoast &lt;- coast %&gt;% \n  st_transform(crs(sst_2008))\n\n\n\n\nRead in the depth data using the terra::rast() function.\nIt’s important that when we are trying to work with different datatsets, we make sure that they have the same CRS. Take this opportunity to re-project the depth data to the same CRS as the SST data so it’s easier to manipulate throughout the workflow.\n\n#Read in the depth data and reproject to the same CRS as the SST Data\ndepth &lt;- rast('data/depth.tif') %&gt;% \n  project(crs(sst_2008))\n\nCRS CHECK:\nCheck that the CRS of all the datasets match before moving on\n\n#Check that all the CRS match\nst_crs(sst) == st_crs(depth)\n\n[1] TRUE\n\nst_crs(sst) == st_crs(coast)\n\n[1] TRUE\n\nst_crs(coast) == st_crs(depth)\n\n[1] TRUE\n\n\n\n\n\n\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nsea surface temperature: 11-30°C\ndepth: 0-70 meters below sea level\n\nThe next step is to process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions. We don’t want to change the underlying depth data, so we will need to resample to match the SST data using the nearest neighbor approach.\n\nStart by creating a single layer that has the average sea surface temperature\n\n\n#Create a new raster that has the average SST converted to Celcius\nsst_stac &lt;- mean(sst - 273.15) \n\n#Plot the new stack to check that there is only one map in the output and the temp range is reasoanable for Celcuis \nplot(sst_stac)\n\n\n\n\n\nCrop depth raster to match the extent of the SST raster\n\n#create a bounding box using the extents of the SST data\nbbox &lt;- st_bbox(sst)\n#Crop the depth data to just our area of interest\ndepth &lt;- crop(depth, bbox)\n\nRe-sample the depth data to match the resolution of the SST data using the nearest neighbor approach. You can check that the depth and SST match in resolution, extent, and coordinate reference system by stacking the raster layers\n\n#Resample the depth data, use method = \"near\" to use the nearest neighbor approach\ndepth_resampl &lt;- resample(depth, y = sst_stac, method = \"near\")\n\n#Check that the depth data was resampled by stacking the rasters\nsst_depth &lt;- c(sst_stac,depth_resampl)\n\n#Check our stacked dataset by plotting\nplot(sst_depth)\n\n\n\n\n\n\n\nIn order to find suitable locations for marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth. We can achieve this by reclassifying the SST and depth data into locations that are suitable for oysters.\n\n#Create a reclassification matrix for stuitable temperatures\nrcl_temp &lt;- matrix(c(-Inf, 11, NA,\n                     11, 30, 1,\n                     30, Inf, NA ), ncol = 3, byrow = TRUE)\n\n#Reclassify the temperature data\ntemp_reclass &lt;- classify(sst_stac, rcl = rcl_temp)\n#Plot to see the reclassification worked\nplot(temp_reclass)\n\n\n\n#Create a reclasification matrix for the suitable depth\nrcl_depth &lt;- matrix(c(-Inf, -70, NA,\n                      -70, 0 , 1,\n                      0, Inf, NA), ncol = 3, byrow = TRUE)\n\n#Reclassify the Depth data\ndepth_reclass &lt;- classify(depth_resampl, rcl = rcl_depth)\n#Plot to see the reclassification worked\nplot(depth_reclass)\n\n\n\n\nThen use the laap() function to find the areas that have suitable temperatures and depths for oysters. Using the multiply function inside lapp() creates a raster that has 1s where both conditions are TRUE and 0s when one or more conditions are FALSE.\n\n#create a function to multiply two inputs\nmultiply = function(x,y){\n  x*y\n}\n#Use Lapp to create an overlay of the reclassified data\noyster_cond &lt;- lapp(c(temp_reclass, depth_reclass), fun = multiply)\n#Plot the overlay to check the area\nplot(oyster_cond, col = 'blue')\n\n\n\n\n\n\n\n\nNow that our data is cleaned and wrangled, we can begin our analysis to determine the suitable area per EEZ region. Start by using our oyster_cond variable to create. amaks of suitable areas. Then use the mask to crop the EEZ data.\n\n#transform the coastal data into a raster\ncoast_rast &lt;- rasterize(coast, sst, field = 'rgn')\n#Create a mask using the selected areas for suitable pyster conditions\nmask &lt;- mask(coast_rast, oyster_cond)\n#Use the mask to crop the coastal data to our area of interest (suitable areas)\nEEZ &lt;- crop(mask, coast_rast) \n#Check that the raster was cropped correctly\nplot(EEZ)\n\n\n\n\nNow we can use the cellSize() and zonal() finctions to calculcate the total area of each EEZ and the total suiatble areas for oysters in each EEZ\n\n#Use the cellSize function to fin the area of each cell\nEEZ_area &lt;- cellSize(EEZ)\n\n#Use the zonal function to calculate the sum of the suitable areas in each region \nsuit_zones = zonal(EEZ_area, EEZ, fun = 'sum', na.rm = TRUE) \nsuit_zones\n\n                  rgn       area\n1  Central California 4069876613\n2 Northern California  178026784\n3              Oregon 1074271959\n4 Southern California 3757284868\n5          Washington 2378313748\n\n\n\n#Use the cellSize function to fin the area of each cell\ntotal_area &lt;- cellSize(coast_rast)\n#Use the zonal function to calculate the sum of the total areas in each region \nzones = zonal(total_area, coast_rast, fun = 'sum', na.rm = TRUE)\nzones\n\n                  rgn         area\n1  Central California 202779854223\n2 Northern California 163715001370\n3              Oregon 179866415384\n4 Southern California 206535860068\n5          Washington  67813688037\n\n\nCreate one singular dataframe with the total area per region(km2), total suitable area(km2), and percentage of suitable area(%)\n\n#Add the suitable area stats to the new variable zonal_stats\nzonal_stats &lt;- suit_zones %&gt;% \n  #Rename the area column to suitable area for clarity\n  rename(suitable_area = area) %&gt;% \n  #Convert to km2\n  mutate(suitable_area = (suitable_area/1000000)) %&gt;% \n  #Add a new column ot the dataframe using the total area stats calculated above\n  add_column(zones$area) %&gt;% \n  #Rename that area column to total-area for clarity\n  rename(total_area = 'zones$area') %&gt;% \n  #Convert to km2\n  mutate(total_area = (total_area/1000000)) %&gt;% \n  #Add a new column that calculates the percentage of each region thats suitable\n  mutate(pct_suitable = ((suitable_area/total_area)*100))\n#Print the output of the new dataset\nzonal_stats\n\n                  rgn suitable_area total_area pct_suitable\n1  Central California     4069.8766  202779.85    2.0070419\n2 Northern California      178.0268  163715.00    0.1087419\n3              Oregon     1074.2720  179866.42    0.5972610\n4 Southern California     3757.2849  206535.86    1.8191925\n5          Washington     2378.3137   67813.69    3.5071293\n\n#Add the geometry to the zonal stats dataset by using a left joim \nzonal_rast &lt;- left_join(coast, zonal_stats, by = 'rgn') \n\n\n\nNow that we have results, we need to present them! Use the super fun package tmap to create graph of total suitable area(km2), and percentage of suitable area(%) with updated titles, legends, etc.\n\n#Create a map to plot the suiatable area in km^2\ntm_shape(zonal_rast)+\n  tm_polygons(fill = 'suitable_area', fill_alpha = 0.6, #Add geometry and opacity\n              fill.scale = tm_scale(breaks = c(0,1000,2000,3000,4000,5000), #Add breaks, and legend title\n                                    values = \"YlGn\"), #Add Color scheme\n              fill.legend = tm_legend(title = 'Suitable Area (km^2)'))+ #Add legend title\n  tm_title(text = \"Suitable Area by Region: Oysters\")+ # Add figure title\n  tm_scalebar(position = c('left','bottom'))+ #Add a scalebar\n  tm_basemap(server = \"OpenStreetMap\") #Add a basemap\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"YlGn\" is named\n\"brewer.yl_gn\"\nMultiple palettes called \"yl_gn\" found: \"brewer.yl_gn\", \"matplotlib.yl_gn\". The first one, \"brewer.yl_gn\", is returned.\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n#Create a map to show the percent suitable area\ntm_shape(zonal_rast)+\n  tm_polygons(fill = 'pct_suitable', fill_alpha = 0.7, #Add geometry and opacity\n              fill.scale = tm_scale(breaks = c(0,0.5,1,2,3,4), #Add breaks\n                values = \"YlGn\"), #Add color scheme\n              fill.legend = tm_legend(title = 'Suitable Area(%)'))+ #Add legend title\n  tm_title(text = \"Suitable Area by Region: Oysters\")+ #Add figure title\n  tm_scalebar(position = c('left','bottom'))+ #Add a scalebar\n  tm_basemap(server = \"OpenStreetMap\") #Add a basemap\n\n\n\n\n\n\n\nNow that we’ve developed a workflow for one group of species, we can update the workflow to create a function that would allow you to reproduce your results for other species.\nRun the function for a species of your choice! You can find information on species depth and temperature requirements on SeaLifeBase.\n\nspecies_suitable_area = function(species_name, min_temp_C, max_temp_C, min_depth_m, max_depth_m){\n  #Create a reclassification matrix for Temperature\n  rcl_temp &lt;- matrix(c(-Inf, min_temp_C, NA,\n                     min_temp_C, max_temp_C, 1,\n                     max_temp_C, Inf, NA ), ncol = 3, byrow = TRUE)\n  #Reclassify the temp data\n  temp_reclass &lt;- classify(sst_stac, rcl = rcl_temp)\n  #Create a reclassification matrix for depth\n  rcl_depth &lt;- matrix(c(-Inf, (max_depth_m*(-1)), NA,\n                      (max_depth_m*(-1)), (min_depth_m*(-1)), 1,\n                      (min_depth_m*(-1)), Inf, NA), ncol = 3, byrow = TRUE)\n  #reclassify the depth data\n  depth_reclass &lt;- classify(depth_resampl, rcl = rcl_depth)\n    #Create a function to multiply two outputs\n    multiply = function(x,y){\n    x*y\n    }\n    #Use lapp() to create an overlay from the product of the two reclassified datasets\n  species_cond &lt;- lapp(c(temp_reclass, depth_reclass), fun = multiply)\n  #Make a mask from the overlay\n  mask &lt;- mask(coast_rast, species_cond)\n  #Use the mask to crop the coastal data to our area of interest (suitable areas)\n  EEZ &lt;- crop(mask, coast_rast) \n  #Use the cellSize function to fin the area of each cell\n  EEZ_area &lt;- cellSize(EEZ)\n  #Use the zonal function to calculate the sum of the suitable areas in each region \n  suit_zones = zonal(EEZ_area, EEZ, fun = 'sum', na.rm = TRUE) \n  #Use the cellSize function to fin the area of each cell\n  total_area &lt;- cellSize(coast_rast)\n  #Use the zonal function to calculate the sum of the total areas in each region \n  fun_zones &lt;&lt;- zonal(total_area, coast_rast, fun = 'sum', na.rm = TRUE)\n  \n  #Add the suitable area stats to the new variable zonal_stats\n  zonal_stats &lt;- suit_zones %&gt;% \n  #Rename the area column to suitable area for clarity\n  rename(suitable_area = area) %&gt;% \n  #Convert to km2\n  mutate(suitable_area = (suitable_area/1000000)) %&gt;% \n  #Add a new column to the dataframe using the total area stats calculated above\n  add_column(fun_zones$area) %&gt;% \n  #Rename that area column to total-area for clarity\n  rename(total_area = 'fun_zones$area') %&gt;% \n  #Convert to km2\n  mutate(total_area = (total_area/1000000)) %&gt;% \n  #Add a new column that calculates the percentage of each region thats suitable\n  mutate(pct_suitable = ((suitable_area/total_area)*100))\n\n  #Add the geometry to the zonal stats dataset by using a left joim \n  zonal_rast &lt;- left_join(coast, zonal_stats, by = 'rgn') \n  \n  area_plot&lt;- tm_shape(zonal_rast)+\n  tm_polygons(fill = 'suitable_area', fill_alpha = 0.6, #Add geometry and opacity\n              fill.scale = tm_scale(values = \"YlGn\"), #Add Color scheme\n              fill.legend = tm_legend(title = 'Suitable Area (km^2)'))+ #Add legend title\n  tm_title(text = paste0(\"Suitable Area by Region: \", species_name))+ # Add figure title\n  tm_scalebar(position = c('left','bottom'))+ #Add a scalebar\n  tm_basemap(server = \"OpenStreetMap\") #Add a basemap\n  \n  pct_plot &lt;- tm_shape(zonal_rast)+\n  tm_polygons(fill = 'pct_suitable', fill_alpha = 0.7, #Add geometry and opacity\n              fill.scale = tm_scale(values = \"YlGn\"), #Add color scheme\n              fill.legend = tm_legend(title = 'Suitable Area(%)'))+ #Add legend title\n  tm_title(text = paste0(\"Suitable Area by Region: \", species_name))+ #Add figure title\n  tm_scalebar(position = c('left','bottom'))+ #Add a scalebar\n  tm_basemap(server = \"OpenStreetMap\") #Add a basemap\n  \n  return(list(area_plot,pct_plot))\n\n}"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-9/index.html#overview",
    "href": "Geospatial_Blogs/2023-12-9/index.html#overview",
    "title": "Quantifying Suitable Growth Area for Aquaculture Species Along the West Coast",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production. Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, bottom depth .\nThis exercise uses suitable temperature and depth data for each Exclusive Economic Zones (EEZ) on the West Coast of the US to find the area that are best suited to developing marine aquaculture for several species of oysters.\nThis workflow is then expanded to create a function that can take in temperature and depth limits and produce graphs that show the area in square kilometers and the percentage of each EEZ that is suitable for that species’ suitable conditions.\nThis repository also contains an R script with the finalized function inside.\n\n\n\nCombining vector/raster data\nRe-sampling raster data\nMasking raster data\nMap algebra"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-9/index.html#data",
    "href": "Geospatial_Blogs/2023-12-9/index.html#data",
    "title": "Quantifying Suitable Growth Area for Aquaculture Species Along the West Coast",
    "section": "",
    "text": "To download the data used for this project, check out the link in my GitHub Repository!\n\n\nWe will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\n\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org.\n\n\n\nTo characterize the depth of the ocean we will use the General Bathymetric Chart of the Oceans (GEBCO).1"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-9/index.html#workflow",
    "href": "Geospatial_Blogs/2023-12-9/index.html#workflow",
    "title": "Quantifying Suitable Growth Area for Aquaculture Species Along the West Coast",
    "section": "",
    "text": "library(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(tmap)\nlibrary(cowplot)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(devtools)\nlibrary(tmaptools)\nlibrary(maptiles)\n\n\n\n\n\n\nRead in the SST Data using the terra::rast() function. Make sure you are in the correct working directory when reading in this data. If you are having issues, you can download my Rmarkdown file from my GitHub repository which uses here() to make this workflow more reproducible, assuming you followed the same folder structure outlined in the repository README.\n\n#Read in each of the Annual SSTs from 2008-2012\nsst_2008 &lt;- rast('data/average_annual_sst_2008.tif')\nsst_2009 &lt;- rast('data/average_annual_sst_2009.tif')\nsst_2010 &lt;- rast('data/average_annual_sst_2010.tif')\nsst_2011 &lt;- rast('data/average_annual_sst_2011.tif')\nsst_2012 &lt;- rast('data/average_annual_sst_2012.tif')\n\n#Create a rasteer stack from the 5 individual rasters\nsst &lt;- c(sst_2008,\n         sst_2009,\n         sst_2010,\n         sst_2011,\n         sst_2012)\n\n#Plot the raster stack to make sure the data was read in and stacked properly\nplot(sst)\n\n\n\n\n\n\n\nRead in the EEZ data using the sf::st_read() function.\nIt’s important that when we are trying to work with different datatsets, we make sure that they have the same CRS. Take this opportunity to re-project the EEZ data to the same CRS as the SST data so it’s easier to manipulate throughout the workflow.\n\ncoast &lt;- st_read('data/wc_regions_clean.shp')\n\n#Reproject the coastal data to the same CRS as the SST Data\ncoast &lt;- coast %&gt;% \n  st_transform(crs(sst_2008))\n\n\n\n\nRead in the depth data using the terra::rast() function.\nIt’s important that when we are trying to work with different datatsets, we make sure that they have the same CRS. Take this opportunity to re-project the depth data to the same CRS as the SST data so it’s easier to manipulate throughout the workflow.\n\n#Read in the depth data and reproject to the same CRS as the SST Data\ndepth &lt;- rast('data/depth.tif') %&gt;% \n  project(crs(sst_2008))\n\nCRS CHECK:\nCheck that the CRS of all the datasets match before moving on\n\n#Check that all the CRS match\nst_crs(sst) == st_crs(depth)\n\n[1] TRUE\n\nst_crs(sst) == st_crs(coast)\n\n[1] TRUE\n\nst_crs(coast) == st_crs(depth)\n\n[1] TRUE\n\n\n\n\n\n\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nsea surface temperature: 11-30°C\ndepth: 0-70 meters below sea level\n\nThe next step is to process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions. We don’t want to change the underlying depth data, so we will need to resample to match the SST data using the nearest neighbor approach.\n\nStart by creating a single layer that has the average sea surface temperature\n\n\n#Create a new raster that has the average SST converted to Celcius\nsst_stac &lt;- mean(sst - 273.15) \n\n#Plot the new stack to check that there is only one map in the output and the temp range is reasoanable for Celcuis \nplot(sst_stac)\n\n\n\n\n\nCrop depth raster to match the extent of the SST raster\n\n#create a bounding box using the extents of the SST data\nbbox &lt;- st_bbox(sst)\n#Crop the depth data to just our area of interest\ndepth &lt;- crop(depth, bbox)\n\nRe-sample the depth data to match the resolution of the SST data using the nearest neighbor approach. You can check that the depth and SST match in resolution, extent, and coordinate reference system by stacking the raster layers\n\n#Resample the depth data, use method = \"near\" to use the nearest neighbor approach\ndepth_resampl &lt;- resample(depth, y = sst_stac, method = \"near\")\n\n#Check that the depth data was resampled by stacking the rasters\nsst_depth &lt;- c(sst_stac,depth_resampl)\n\n#Check our stacked dataset by plotting\nplot(sst_depth)\n\n\n\n\n\n\n\nIn order to find suitable locations for marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth. We can achieve this by reclassifying the SST and depth data into locations that are suitable for oysters.\n\n#Create a reclassification matrix for stuitable temperatures\nrcl_temp &lt;- matrix(c(-Inf, 11, NA,\n                     11, 30, 1,\n                     30, Inf, NA ), ncol = 3, byrow = TRUE)\n\n#Reclassify the temperature data\ntemp_reclass &lt;- classify(sst_stac, rcl = rcl_temp)\n#Plot to see the reclassification worked\nplot(temp_reclass)\n\n\n\n#Create a reclasification matrix for the suitable depth\nrcl_depth &lt;- matrix(c(-Inf, -70, NA,\n                      -70, 0 , 1,\n                      0, Inf, NA), ncol = 3, byrow = TRUE)\n\n#Reclassify the Depth data\ndepth_reclass &lt;- classify(depth_resampl, rcl = rcl_depth)\n#Plot to see the reclassification worked\nplot(depth_reclass)\n\n\n\n\nThen use the laap() function to find the areas that have suitable temperatures and depths for oysters. Using the multiply function inside lapp() creates a raster that has 1s where both conditions are TRUE and 0s when one or more conditions are FALSE.\n\n#create a function to multiply two inputs\nmultiply = function(x,y){\n  x*y\n}\n#Use Lapp to create an overlay of the reclassified data\noyster_cond &lt;- lapp(c(temp_reclass, depth_reclass), fun = multiply)\n#Plot the overlay to check the area\nplot(oyster_cond, col = 'blue')\n\n\n\n\n\n\n\n\nNow that our data is cleaned and wrangled, we can begin our analysis to determine the suitable area per EEZ region. Start by using our oyster_cond variable to create. amaks of suitable areas. Then use the mask to crop the EEZ data.\n\n#transform the coastal data into a raster\ncoast_rast &lt;- rasterize(coast, sst, field = 'rgn')\n#Create a mask using the selected areas for suitable pyster conditions\nmask &lt;- mask(coast_rast, oyster_cond)\n#Use the mask to crop the coastal data to our area of interest (suitable areas)\nEEZ &lt;- crop(mask, coast_rast) \n#Check that the raster was cropped correctly\nplot(EEZ)\n\n\n\n\nNow we can use the cellSize() and zonal() finctions to calculcate the total area of each EEZ and the total suiatble areas for oysters in each EEZ\n\n#Use the cellSize function to fin the area of each cell\nEEZ_area &lt;- cellSize(EEZ)\n\n#Use the zonal function to calculate the sum of the suitable areas in each region \nsuit_zones = zonal(EEZ_area, EEZ, fun = 'sum', na.rm = TRUE) \nsuit_zones\n\n                  rgn       area\n1  Central California 4069876613\n2 Northern California  178026784\n3              Oregon 1074271959\n4 Southern California 3757284868\n5          Washington 2378313748\n\n\n\n#Use the cellSize function to fin the area of each cell\ntotal_area &lt;- cellSize(coast_rast)\n#Use the zonal function to calculate the sum of the total areas in each region \nzones = zonal(total_area, coast_rast, fun = 'sum', na.rm = TRUE)\nzones\n\n                  rgn         area\n1  Central California 202779854223\n2 Northern California 163715001370\n3              Oregon 179866415384\n4 Southern California 206535860068\n5          Washington  67813688037\n\n\nCreate one singular dataframe with the total area per region(km2), total suitable area(km2), and percentage of suitable area(%)\n\n#Add the suitable area stats to the new variable zonal_stats\nzonal_stats &lt;- suit_zones %&gt;% \n  #Rename the area column to suitable area for clarity\n  rename(suitable_area = area) %&gt;% \n  #Convert to km2\n  mutate(suitable_area = (suitable_area/1000000)) %&gt;% \n  #Add a new column ot the dataframe using the total area stats calculated above\n  add_column(zones$area) %&gt;% \n  #Rename that area column to total-area for clarity\n  rename(total_area = 'zones$area') %&gt;% \n  #Convert to km2\n  mutate(total_area = (total_area/1000000)) %&gt;% \n  #Add a new column that calculates the percentage of each region thats suitable\n  mutate(pct_suitable = ((suitable_area/total_area)*100))\n#Print the output of the new dataset\nzonal_stats\n\n                  rgn suitable_area total_area pct_suitable\n1  Central California     4069.8766  202779.85    2.0070419\n2 Northern California      178.0268  163715.00    0.1087419\n3              Oregon     1074.2720  179866.42    0.5972610\n4 Southern California     3757.2849  206535.86    1.8191925\n5          Washington     2378.3137   67813.69    3.5071293\n\n#Add the geometry to the zonal stats dataset by using a left joim \nzonal_rast &lt;- left_join(coast, zonal_stats, by = 'rgn') \n\n\n\nNow that we have results, we need to present them! Use the super fun package tmap to create graph of total suitable area(km2), and percentage of suitable area(%) with updated titles, legends, etc.\n\n#Create a map to plot the suiatable area in km^2\ntm_shape(zonal_rast)+\n  tm_polygons(fill = 'suitable_area', fill_alpha = 0.6, #Add geometry and opacity\n              fill.scale = tm_scale(breaks = c(0,1000,2000,3000,4000,5000), #Add breaks, and legend title\n                                    values = \"YlGn\"), #Add Color scheme\n              fill.legend = tm_legend(title = 'Suitable Area (km^2)'))+ #Add legend title\n  tm_title(text = \"Suitable Area by Region: Oysters\")+ # Add figure title\n  tm_scalebar(position = c('left','bottom'))+ #Add a scalebar\n  tm_basemap(server = \"OpenStreetMap\") #Add a basemap\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"YlGn\" is named\n\"brewer.yl_gn\"\nMultiple palettes called \"yl_gn\" found: \"brewer.yl_gn\", \"matplotlib.yl_gn\". The first one, \"brewer.yl_gn\", is returned.\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n#Create a map to show the percent suitable area\ntm_shape(zonal_rast)+\n  tm_polygons(fill = 'pct_suitable', fill_alpha = 0.7, #Add geometry and opacity\n              fill.scale = tm_scale(breaks = c(0,0.5,1,2,3,4), #Add breaks\n                values = \"YlGn\"), #Add color scheme\n              fill.legend = tm_legend(title = 'Suitable Area(%)'))+ #Add legend title\n  tm_title(text = \"Suitable Area by Region: Oysters\")+ #Add figure title\n  tm_scalebar(position = c('left','bottom'))+ #Add a scalebar\n  tm_basemap(server = \"OpenStreetMap\") #Add a basemap\n\n\n\n\n\n\n\nNow that we’ve developed a workflow for one group of species, we can update the workflow to create a function that would allow you to reproduce your results for other species.\nRun the function for a species of your choice! You can find information on species depth and temperature requirements on SeaLifeBase.\n\nspecies_suitable_area = function(species_name, min_temp_C, max_temp_C, min_depth_m, max_depth_m){\n  #Create a reclassification matrix for Temperature\n  rcl_temp &lt;- matrix(c(-Inf, min_temp_C, NA,\n                     min_temp_C, max_temp_C, 1,\n                     max_temp_C, Inf, NA ), ncol = 3, byrow = TRUE)\n  #Reclassify the temp data\n  temp_reclass &lt;- classify(sst_stac, rcl = rcl_temp)\n  #Create a reclassification matrix for depth\n  rcl_depth &lt;- matrix(c(-Inf, (max_depth_m*(-1)), NA,\n                      (max_depth_m*(-1)), (min_depth_m*(-1)), 1,\n                      (min_depth_m*(-1)), Inf, NA), ncol = 3, byrow = TRUE)\n  #reclassify the depth data\n  depth_reclass &lt;- classify(depth_resampl, rcl = rcl_depth)\n    #Create a function to multiply two outputs\n    multiply = function(x,y){\n    x*y\n    }\n    #Use lapp() to create an overlay from the product of the two reclassified datasets\n  species_cond &lt;- lapp(c(temp_reclass, depth_reclass), fun = multiply)\n  #Make a mask from the overlay\n  mask &lt;- mask(coast_rast, species_cond)\n  #Use the mask to crop the coastal data to our area of interest (suitable areas)\n  EEZ &lt;- crop(mask, coast_rast) \n  #Use the cellSize function to fin the area of each cell\n  EEZ_area &lt;- cellSize(EEZ)\n  #Use the zonal function to calculate the sum of the suitable areas in each region \n  suit_zones = zonal(EEZ_area, EEZ, fun = 'sum', na.rm = TRUE) \n  #Use the cellSize function to fin the area of each cell\n  total_area &lt;- cellSize(coast_rast)\n  #Use the zonal function to calculate the sum of the total areas in each region \n  fun_zones &lt;&lt;- zonal(total_area, coast_rast, fun = 'sum', na.rm = TRUE)\n  \n  #Add the suitable area stats to the new variable zonal_stats\n  zonal_stats &lt;- suit_zones %&gt;% \n  #Rename the area column to suitable area for clarity\n  rename(suitable_area = area) %&gt;% \n  #Convert to km2\n  mutate(suitable_area = (suitable_area/1000000)) %&gt;% \n  #Add a new column to the dataframe using the total area stats calculated above\n  add_column(fun_zones$area) %&gt;% \n  #Rename that area column to total-area for clarity\n  rename(total_area = 'fun_zones$area') %&gt;% \n  #Convert to km2\n  mutate(total_area = (total_area/1000000)) %&gt;% \n  #Add a new column that calculates the percentage of each region thats suitable\n  mutate(pct_suitable = ((suitable_area/total_area)*100))\n\n  #Add the geometry to the zonal stats dataset by using a left joim \n  zonal_rast &lt;- left_join(coast, zonal_stats, by = 'rgn') \n  \n  area_plot&lt;- tm_shape(zonal_rast)+\n  tm_polygons(fill = 'suitable_area', fill_alpha = 0.6, #Add geometry and opacity\n              fill.scale = tm_scale(values = \"YlGn\"), #Add Color scheme\n              fill.legend = tm_legend(title = 'Suitable Area (km^2)'))+ #Add legend title\n  tm_title(text = paste0(\"Suitable Area by Region: \", species_name))+ # Add figure title\n  tm_scalebar(position = c('left','bottom'))+ #Add a scalebar\n  tm_basemap(server = \"OpenStreetMap\") #Add a basemap\n  \n  pct_plot &lt;- tm_shape(zonal_rast)+\n  tm_polygons(fill = 'pct_suitable', fill_alpha = 0.7, #Add geometry and opacity\n              fill.scale = tm_scale(values = \"YlGn\"), #Add color scheme\n              fill.legend = tm_legend(title = 'Suitable Area(%)'))+ #Add legend title\n  tm_title(text = paste0(\"Suitable Area by Region: \", species_name))+ #Add figure title\n  tm_scalebar(position = c('left','bottom'))+ #Add a scalebar\n  tm_basemap(server = \"OpenStreetMap\") #Add a basemap\n  \n  return(list(area_plot,pct_plot))\n\n}"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-9/index.html#footnotes",
    "href": "Geospatial_Blogs/2023-12-9/index.html#footnotes",
    "title": "Quantifying Suitable Growth Area for Aquaculture Species Along the West Coast",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGEBCO Compilation Group (2022) GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c).↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "It was impossible to live in Santa Barbara and not be captivated by the environmental and ecological diversity surrounding me. I saw firsthand how our built world interacts with nature through the impact of human activities on ecosystems and how science can be used as a tool for sustainability. This awareness led me to explore biological and agricultural sciences, recognizing the critical role of terrestrial systems in addressing climate challenges. I became fascinated with how scientific advancements in agriculture, ecology, and sustainability could be used to protect the very landscapes I grew up admiring.\nMy passion for STEM was ignited at the Dos Pueblos Engineering Academy (DPEA), an immersive program that challenged me to think critically, solve problems creatively, and develop hands-on engineering skills. From designing mechanical systems to collaborating on team projects, I thrived in an environment that encouraged curiosity, technical expertise, and innovation. These experiences laid the foundation for my ability to tackle complex challenges; a skill I carry with me in my research today.\nBeyond academics, my time at Dos Pueblos High School instilled in me a strong sense of community and responsibility. I was encouraged to use my successes not just for personal growth but as a way to uplift others. By my senior year of high school, I was a teaching assistant for two class periods a day, working directly with students to build their confidence and skills in STEM. I also scheduled regular tutoring sessions in statistics, biology, algebra, and Spanish through bi-weekly seminars, helping to create a more inclusive and supportive learning environment.\n\nCultivating Knowledge and Community at UC Davis\nWhen I discovered the Biological and Agricultural Engineering (BAE) Department at UC Davis, I knew I had found a place where I could continue to explore problem-solving at the intersection of my two passions: life sciences and engineering. My academic journey in the Biological Systems Engineering major culminated in my team of undergraduate students completing an 11-month capstone project developing a solid-state fermentation bioreactor to scale up the production of protein-enriched animal feed from almond hulls. Our innovative design optimized the resource intensity of the fermentation process and increased protein content to reduce food waste and utilize the final product in alternative markets.\nWhile smaller than some other engineering departments at UC Davis, the Biological and Agricultural Engineering Department shared many community values of collaboration and service that I grew up with. This encouraged me to pursue leadership positions throughout the department, including the President of the Society of Biological Engineers, a BAE ambassador, and the Undergraduate Representative for the graduating class of 2022. Through these positions, I hosted department-wide events to strengthen our community and build connections between staff and students, organized social and study hours to connect students and encourage student involvement, and presented to the BAE Leadership Board to provide insight into the student experience and suggest future improvements.\nWhen I had the opportunity to take elective classes, I was always drawn to learning more about earth systems and atmospheric sciences. By the time I graduated, I had taken enough additional courses that I was able to graduate with a minor in atmospheric science. While my major prepared me for problem-solving in the real world, a minor in atmospheric science left me eager to learn more. I couldn’t help but feel that crucial problems were waiting to be solved, but I lacked the modeling and data analysis skills to bring those solutions to life. \n\n\nDeepening My Passions at the Bren School of Environmental Science & Management and UC Santa Barbara\nMy educational journey led me back to my loving home in Santa Barbara where I where I completed a Master of Environmental Data Science (MEDS) degree from the Bren School of Environmental Science & Management at UC Santa Barbara. When I applied, I knew this program checked all the boxes for me as an opportunity to learn fundamental coding skills in R, Python, and SQL in an environmentally focused, interdisciplinary program with strong community values and opportunities for leadership.\nAt Bren, I developed a deep understanding of how scientific research, data-driven solutions, and interdisciplinary collaboration could drive real-world environmental impact. My coursework and projects allowed me to apply fundamentals in geospatial analysis, machine learning, and data visualization to applications in mapping, climate modeling, and environmental impact assessment while incorporating elements of reproducibility, environmental justice and best practices for version control in collaborative coding. These elements came together in my Master’s Capstone Project: Understanding the Influence of Parameter Value Uncertainty on Climate Model Output (more info about this project can be found on this page).\nOutside of the classroom, Bren strengthened my commitment to mentorship and leadership. As class co-chair, a member of the Dean’s Advisory Council, and social co-chair for the Women in Science and Engineering (WISE) Club, I worked to foster a strong, inclusive community. I found joy in creating spaces for students to connect, grow, and thrive, ensuring that mentorship and support were integral parts of our academic experience."
  },
  {
    "objectID": "xtra.html",
    "href": "xtra.html",
    "title": "Leadership",
    "section": "",
    "text": "Society of Biological Engineers Alumni Advisor\n\nDept. of Bio & Ag Engineering at UC Davis (9/25–Present)\nCoordinated with the department chair, faculty advisor, and organizing board to understand the needs of the Biological Systems Engineering student body for professional development workshops\nCurated original material specific to the coursework completed by the Biological Systems Engineering students to help them navigate their career paths, networking skills, and plans for higher education\n\n\n\nDean's Advisory Council Member\n\nBren School of Environmental Science & Management (9/23–6/24)\nCondensed live feedback and survey results from active students to provide the Dean with summaries of key concerns and actionable solutions, ensuring student satisfaction for current and future cohorts\nCultivated open communication between the Dean, staff, and students to improve the student experience\n\n\n\nMEDS Class of 2024 Co-chair\n\nBren School of Environmental Science & Management (9/23–6/24)\nCoordinated with the Bren Student Leadership Committee to align students on pressing issues\nOrganized community-building events and assisted Bren staff with outreach and fundraising events for improvements and diversity scholarships\nEffectively managed multiple schedules by maintaining the MEDS Assignments Google Calendar, ensuring students were up to date on class deadlines, events, and important announcements\n\n\n\nWomxn in Science and Engineering Social Chair\n\nUniversity of California, Santa Barbara (5/23–6/24)\nAnonymized and reviewed annual fellowship applications of graduate student summer funding for the UC Santa Barbara Graduate Division to facilitate equitable distribution of funding to women in STEM\nOrganized regular club events, including making flyers and Instagram posts, reserving spaces, and coordinating presenters, all while working within a budget set by the finance chair\nPlanned outreach events for youth and women in STEM at local high schools, museums, and other Santa Barbara-based Women in STEM events\n\n\n\nSociety of Biological Engineers President and Events Coordinator\n\nUC Davis (6/20–6/22)\nSpearheaded the club's transition from virtual to in-person events while following safety guidelines\nOrganized various events, including Barbecues, bowling, study sessions, game nights, bike rides, industry speakers, and collaborations with the Graduate Student Association and the Internship and Career Center\nRegistered the club with all officers and current members to the university, attended informational sessions for club leaders, and filled out necessary forms for participation in club outreach events\nResponsible for documenting club purchases for events and placing/receiving orders for marketing materials\n\n\n\nUndergraduate Representative for the Biological System Engineering Major\n\nUC Davis (9/22–6/22)\nPresented to the Biological and Agricultural Engineering Department Leadership Board regarding student involvement, employment opportunities on campus and after graduation\nImplemented solutions based on student feedback, including providing the senior class with custom stoles for graduation and creating individual certificates for the department graduation brunch\n\n\n\nStudent Ambassador for the Biological & Agricultural Engineering Department\n\nUC Davis (9/22–6/22)\nCollaborated with the department’s outreach team to work on increasing their social media presence through student Q and A posts, Instagram takeovers, TikToks, and upcoming event information\nProvided one-on-one peer mentoring for incoming freshmen and transfer students about getting involved on campus and setting up a manageable course load each quarter"
  },
  {
    "objectID": "blog_posts/2024-3-16/index.html",
    "href": "blog_posts/2024-3-16/index.html",
    "title": "21st Century Energy Evolution: Trends & Transformations",
    "section": "",
    "text": "How has energy usage/consumption changed over time:\n\nHow has the overall usage changed over time?\nhow have the trends usage per sector changed between the 20th and 21st century?\nWhy are we seeing the trends we are?\n\n\n\n\n\nAll the data and definitions used in this infographic come from the U.S. Energy Information Administration website. The first dataset is annual primary energy consumption by U.S. sector in trillion British thermal units (Btu) from 1949 to 2022. Working with this data required me to combine two datasets that I downloaded separately and filter to the exact subset of data I was after for each plot. The second dataset is annual U.S. energy intensity from 1949 to 2022 in a variety of units. I also used the EIA glossary page to find variable descriptions on an as needed basis.\n\n\n\ngraphic form: I found that the best way to display this data was with the fundamental chart types: area chart, line graph, and scatterplot. When looking at the data-to-viz website, there aren’t too many “creative” graphic forms to display trends over time. From the other options I found, including boxplots and ridgeline plots, the line and area plots made the most sense for this dataset, especially when including relevant linear regressions.\ntext (e.g. titles, captions, annotations, axis labels, axis ticks, alt text): For each of the figures below I was careful and considerate when choosing which text elements to include. Each of my figures include a title that gives the reader a quick overview of what data they are looking at. I chose not to include the take-home message in the title of the plot because I believe that information should be stored in the text of the infographic. Similarly with subtitles, only the energy intensity plot has a subtitle which describes what energy intensity is because it is not a commonly-known term. I generally would put the take-home message in the subtitle, but again that’s why there is accompanying text on the infographic. For captions, two of the graphs have captions with the data source included. The four middle graphs use the same dataset as cited in the very first graph and it felt redundant to include the same citations five times on the same infographic. Each figure as alt-text that describes the type of plot, the data being displayed, and the main message of the plot. For the two graphs that have captions the alt text is written into the code, and the four graphs that didn’t have captions in the infographic have captions and alt-text that I added in when I embedded the image into the site page. The full embedded inforgraphic also has alt-text. For axes properties, I made all the axes the same color as the background of the infographic for cohesion, and I used consitent scaling of the y-axis within each sector for easy comparison.\nthemes: I started each graph with theme_bw() and modified the elements from there including: removing legends, removing gridlines, removing panel border, and adjusting text. I decided to remove the gridlines from most of the plots because any plot showing trends doesn’t really need the gridlines, and with the linear regression plots that was just way too many lines to have on one small plot. I did decide to leave the major gridlines on the energy intensity plot because I feel like it’s nice to have an idea of roughly which time periods had the 15-10, 10-5, and &lt; 5, energy intensities. Since this infographic has a ton of plots, I used the large icons representing each of the sectors to help orient the reader before they get into the details of the graph. I feel like without reading much of the figure text, you can tell which sector is which and see there are clear changes in usage-trends for all four sectors.\ncolors: I was really stumped about which colors to use because in a perfect world the colors would help the reader identify the sector, but when I think of electricity I think of yellow and when I think of each sector, I basically get varying shades of grey. From personal choice, I hate looking at yellow on a grey background on a screen, it gives me an instant migraine. So I went with a dark blue background and used white icons and text blocks to highlight the plots, and used dark blue text throughout the infographic. I also kept the colors consistent throughout so each sector is always represented as the same color and I used the accent lines color (same hue darker shade of each color in the color palette) on the stacked area chart as the trendline color for the sector graphs.\ntypography: I used a serif/san-serif typeface pairing using Playfair Display and Nunito. I used Playfair Display for all of the chart and infographic titles to capture the more stylistic font on the visualization headlines. I utilized the san-serif font, Nunito, for its readability when I needed small font on graph axes labels, or when I had blocks of text in the infographic.\ngeneral design (e.g. group order, spacing, text orientation, data-ink ratio, creating a visual hierarchy, avoiding information overload): There is a ton of information on this infographic. I tried to combat the information overload by including these large, simple icons that help the reader break down the infographic into easily-digestible components. I also have my information organized in a beginning-middle-end format because I was trying to answer a question but I really took it in more of a storytelling direction. So I have the block at the top of the infographic that kind of sets the scene as the introduction with the data source, some context, and a super easy to digest graph. The middle then gets more creative and has more of the nitty-gritty data analysis, and at the end I have another graph with a text block that summarizes why we saw the trends that we did.\ncontextualizing your data: I added context to my data by weaving some of the key definitions and information about the data into the infographic so that readers don’t have to go to a new tab to look up the information they’d need to understand the figure. Hopefully, this makes this high level energy data more accessible to a wider audience.\ncentering your primary message: This is the part where I feel like I stumbled the most. I think I got caught up in the “storytelling” element and had a hard time really hitting the “each graph answers a clear individual question” part of the project. I tried to literally center the bulk of the data analysis and draw the reader in with the visual elements, but I’m hoping that story-line feel helps leave viewers with a lasting conclusion.\nconsidering accessibility (e.g. colorblind-friendly palettes / contrast, alt text) As I mentioned above, I was able to include alt-text for all of the figures on this site. And I used the Coolors website with the Let’s Get Colorblind Google Chrome extension to help me pick a color palette that was color-blind friendly.\napplying a DEI lens to your design (e.g. considering the people / communities / places represented in your data, consider how you frame your questions / issue) Using national level data that was collected by a government agency takes the people/communities element out of the project, but it does open up opportunities for bias. However by reporting my sources and letting the data tell me the story, I hope to have mitigated Equity/Integrity issues.\n\n\n\n\n\nShow the code\n#select relevant Data\nres20 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Residential Sector\" & year &lt; 2000)\n\nres21 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Residential Sector\" & year &gt;= 2000)\n# Perform linear regression\nres20lm &lt;- lm(Value ~ year, data = res20)\nres21lm &lt;- lm(Value ~ year, data = res21)\n#select relevant Data\ncomm20 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Commercial Sector\" & year &lt; 2000)\n\ncomm21 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Commercial Sector\" & year &gt;= 2000)\n# Perform linear regression\ncom20lm &lt;- lm(Value ~ year, data = comm20)\ncom21lm &lt;- lm(Value ~ year, data = comm21)\n#select relevant Data\nind20 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Industrial Sector\" & year &lt; 2000)\n\nind21 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Industrial Sector\" & year &gt;= 2000)\n# Perform linear regression\nind20lm &lt;- lm(Value ~ year, data = ind20)\nind21lm &lt;- lm(Value ~ year, data = ind21)\n#select relevant Data\ntrans20 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Transportation Sector\" & year &lt; 2000)\n\ntrans21 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Transportation Sector\" & year &gt;= 2000)\n# Perform linear regression\ntr20 &lt;- lm(Value ~ year, data = trans20)\ntr21 &lt;- lm(Value ~ year, data = trans21)\n\n\n\n\n\n\n\nShow the code\np &lt;- sct_data %&gt;% \n  #Create a ggplot, fill based on description\n  ggplot(aes(x = year, y = Value, group = Description, fill = Description))+\n  #make it an area chart\n  geom_area(aes(color = Description), stat = \"identity\")+\n  #set the labels so the legend names aren't so long\n  scale_fill_manual(labels = c(\"Commercial\", \"Industrial\", \n                                 \"Residential\", \"Transportation\"), \n                    #add the color palette\n                    values = main_pal)+\n  scale_color_manual(values = accent_pal,\n                     guide = \"none\")+\n  #Add the chart labels/text\n  labs(title = \"Total Annual Energy Consumption by Sector from 1962-2022\",\n       x = \"Year\",\n       y = \"Total Energy Consumption (Trillion Btu)\",\n       caption = \"Source: Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\",\n       alt = \"The figure shows a stacked area chart for annual energy usage from 1962 to 2020 by sector for the Residential, Industrial, Commercial, and Transportation sectors in the United States. The graphs shows a gradual increase in the annual energy usage across all sectors until 2000, when the usage across all sectors begins to plateau. The total energy consumption of all sectors was just below 50,000 Trillion BTU in 1962 and plateaus around 75,000 BTU by 2022. The industrail and transportation setcors use the most energy with the commercial and residential sectors using slightly less overall.\")+\n  #expand to fill the whole space, and fix axis formatting\n  scale_y_continuous(expand = c(0, 0), labels = label_comma())+\n  scale_x_continuous(expand = c(0, 0))+\n  #Edit theme\n  theme_bw()+\n    theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #Remove Minor gridlines\n        panel.border = element_blank(), #Remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #Change the axis line width and color\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1),#Change the axis tick linewidth and color\n        legend.position = \"top\",  # Place legend at the top\n        legend.box = \"horizontal\",  # Make legend items horizontal\n        legend.direction = \"horizontal\",  # Set legend direction to horizontal\n        legend.title = element_blank(), #Remove legend title\n        plot.title = element_text(family=\"playfair\", #change title font\n                                  size=65,  #change title size\n                                  hjust = 0.5, #change title position\n                                  color = \"#264653\"), #change title color\n        plot.caption = element_text(family = \"nunito\", #Change axis font\n                              size = 25,#change caption font size\n                              face = \"bold\", #change caption font face\n                              color = \"#264653\",#change caption color\n                              hjust = 0.5), #center caption\n        axis.text = element_text(family = \"nunito\", #Change axis font\n                              size = 30,#change axis font size\n                              face = \"bold\", #change axis font face\n                              color = \"#264653\"),#change axis color\n        axis.title = element_text(family = \"playfair\", #Change axis title font\n                              size = 40, #Change axis title font size\n                              color = \"#264653\"), #Change axis title font color\n        legend.text = element_text(family = \"nunito\",  #Change legend font\n                                   size = 35, #Change legend font size\n                                   color = \"#264653\"), #Change legend font color\n        #Create space between the graph and the x axis\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")), \n        #Create space between the graph and the y axis\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n#ggsave(filename = \"infographic_plot1.png\", width = 16, height = 4)\n\n\n\n\n\n\n\n\nShow the code\n#------------------------------------------------------------------------------\n#                              Residential Sector                         \n#-------------------------------------------------------------------------------\n#Select Relevant Data\nPlot2.1a &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Residential Sector\") %&gt;% \n  filter(year &lt; 2000) %&gt;% \n#Create Plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#93B7BE\")+\n  #add the labels for the whole chart\n  labs(title = \"20th Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Residential Energy Consumption (Trillion Btu)\")+\n# Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = custom_breaks, #set custom breaks\n                     limits = custom_limits, #set custom limits\n                     labels = label_comma())+ #format axis labels\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#456B73\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #remove minor gridlines\n        panel.border = element_blank(), #remove border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #Set axis tick properties\n        legend.position = \"NA\",  # Remove Legend\n        #Set text properties including position, size, and color\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\" ),\n        #Expand Plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Select Relevant Data\nPlot2.1b &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Residential Sector\") %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n#Create Plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#93B7BE\")+\n  #add the labels for the whole chart\n  labs(title = \"21st Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Residential Energy Consumption (Trillion Btu)\")+\n  # Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = custom_breaks, \n                     limits = custom_limits, \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#456B73\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #remove minor gridlines\n        panel.border = element_blank(), #remove panel border,\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #Set axis tick properties\n        legend.position = \"NA\",  # Remove legend\n        #Set text properties including position, size, and color\n                plot.title = element_text(family=\"playfair\",\n                                          size=plot_title_size, \n                                          hjust = 0.5, \n                                          color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        #Expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Plot2.1a +Plot2.1b\n#ggsave(filename = \"infografphic_plot2.1.png\", width = 7, height = 4)\n\n#------------------------------------------------------------------------------\n#                              Industrial Sector                         \n#-------------------------------------------------------------------------------\n\n#Select Relevant Data\nPlot2.3a &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Industrial Sector\") %&gt;% \n  filter(year &lt; 2000) %&gt;% \n#Create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#B24C63\")+\n  #add the labels for the whole chart\n  labs(title = \"20th Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Industrial Energy Consumption (Trillion Btu)\")+\n#Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(25000,30000,35000), \n                     limits = c(20000,40000), \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#471F28\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #Remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1),#set axis tick properties\n        legend.position = \"NA\",  # Remove Legend\n        #Set plot text color, size on font\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        #expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Select relevant data\nPlot2.3b &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Industrial Sector\") %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  #Create Plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#B24C63\")+\n  #add the labels for the whole chart\n  labs(title = \"21st Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Industrial Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(25000,30000,35000), \n                     limits = c(20000,40000), \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#471F28\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(),#Remove minor gridlines\n        panel.border = element_blank(), #Remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #Set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set axis tick properties\n        legend.position = \"NA\",  # Remove legend\n        #Set plot text font, size color and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, color = \"#264653\"),\n        #Expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Plot2.3a + Plot2.3b\n#ggsave(filename = \"infografphic_plot2.3.png\", width = 7, height = 4)\n\n#------------------------------------------------------------------------------\n#                              Commercial Sector                         \n#-------------------------------------------------------------------------------\n#Select Relevant data\nPlot2.2a &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Commercial Sector\") %&gt;% \n  filter(year &lt; 2000) %&gt;% \n  #create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#8AB17D\")+\n  #add the labels for the whole chart\n  labs(title = \"20th Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Commercial Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = custom_breaks, \n                     limits = custom_limits, \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#3E5936\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set axis tick properties\n        legend.position = \"NA\",  # Remove legend\n        #set plot text font, color, size, and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, \n                                color = \"#264653\"),\n        #expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#select relevant Data\nPlot2.2b &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Commercial Sector\") %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  #create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#8AB17D\")+\n  #add the labels for the whole chart\n  labs(title = \"21st Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Commercial Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = custom_breaks, \n                     limits = custom_limits, \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#3E5936\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set asix line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set axis tick properties\n        legend.position = \"NA\",  # remove legend\n        #Set plot text font, color, size, and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, \n                                color = \"#264653\"),\n        #expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Plot2.2a + Plot2.2b\n#ggsave(filename = \"infografphic_plot2.2.png\", width = 7, height = 4)\n#------------------------------------------------------------------------------\n#                              Transportation sector                         \n#-------------------------------------------------------------------------------\n#Select relevant data\nPlot2.4a &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Transportation Sector\") %&gt;% \n  filter(year &lt; 2000) %&gt;% \n  #create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#F28F3B\")+\n  #add the labels for the whole chart\n  labs(title = \"20th Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Transportation Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(15000,20000,25000), \n                     limits = c(10000,30000), \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#864309\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #Remove minor gridlines\n        panel.border = element_blank(), #Remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set axis tick properties\n        legend.position = \"NA\",  # Remove legend\n        #Set plot text size, color, font and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, \n                                color = \"#264653\"),\n        #Expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Select relevant Data\nPlot2.4b &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Transportation Sector\") %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  #Create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#F28F3B\")+\n  #add the labels for the whole chart\n  labs(title = \"21st Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Transportation Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(15000,20000,25000), \n                     limits = c(10000,30000), \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#864309\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #Remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #Set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #Set axis tick properties\n        legend.position = \"NA\",  # Place legend at the top\n        #Set plot text size, color, font, and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 24,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, \n                                color = \"#264653\"),\n        #Expand Plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Plot2.4a + Plot2.4b\n#ggsave(filename = \"infografphic_plot2.4.png\", width = 7, height = 4)\n\n\n\n\n\nSource:Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\n\n\n\n\n\nSource:Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\n\n\n\n\n\nSource:Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\n\n\n\n\n\nSource:Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\n\n\n\n\n\n\n\nShow the code\n#define subtitle text and set the wrapping properties\nsubtitle_text&lt;- str_wrap(\"Energy Intensity is a measure of how much energy is required to produce one unit of Gross Domestic Product. In this chart, energy intensity is measured in Thousand Btu per Chained (2017) Dollar\", width = 100)\ncaption_text &lt;- str_wrap(\"Source: U.S. Energy Information Administration—EIA - Independent Statistics and Analysis. (n.d.). Retrieved March 16, 2024, from https://www.eia.gov/totalenergy/data/browser/index.php?tbl=T01.07#/?f=A&start=1949&end=2020&charted=3\", width = 200)\n#Create the plot\ngraph &lt;- ggplot(data = intensity, aes(x = year, y = Value))+\n  geom_line(color = \"#78309C\", size = 3)+\n  geom_point(color = \"#D5B1E7\", size = 2)+\n  #Add plot labels including caption and alternative text\n    labs(title = \"Energy Intensity in the U.S. from 1949-2022\",\n         subtitle = subtitle_text,\n       x = \"Year\",\n       y = \"Energy Intensity \",\n       caption = caption_text,\n       alt= \"Figure shows a line chart of United States energy intensity from 1949 to 2022. There is a gradual decline in the energy intensity showing a 1949 energy intensity of around 13 Thousand Btu per Chained (2017) Dollar and a 2022 energy intensity of below 5 Thousand Btu per Chained (2017) Dollar, with the steady decrease happening between 1970 and 2022.\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(5,10,15), \n                     limits = c(0,20))+\n  #Set theme properties\n  theme_bw()+\n  theme(\n        panel.grid.minor = element_blank(), #Remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #Set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set the axis tick properties\n        plot.title = element_text(family=\"playfair\", #Set plot title font\n                                  size=50, #Set plot title size\n                                  hjust = 0.25, #Set plot title position\n                                  color = \"#264653\"),#Set plot title font color\n        plot.subtitle = element_text(family=\"nunito\", #Set plot subtitle font\n                                     size=30, #Set plot subtitle font size\n                                     color = \"#264653\", #Set plot subtitle font color\n                                     lineheight = 0.35), #Set plot subtitle linespacing\n        plot.caption = element_text(family = \"nunito\", #Change axis font\n                              size = 16,#change caption font size\n                              face = \"bold\", #change caption font face\n                              color = \"#264653\",#change caption color\n                              hjust = 0.5,\n                              lineheight = 0.35), #center caption\n        axis.text = element_text(family = \"nunito\", #Set axis text font\n                              size = 35, #Set axis text font size\n                              face = \"bold\", #Set axis text font face\n                              color = \"#264653\"), #Set axis text font color\n        axis.title = element_text(family = \"playfair\", #Set axis title font\n                              size = 40, #Set axis title font size\n                              color = \"#264653\"), #Set axis title font color\n        #Add space between the axes and plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\nggsave(filename = \"infografphic_plot3.png\", width = 7, height = 5)"
  },
  {
    "objectID": "blog_posts/2024-3-16/index.html#questions-to-answer",
    "href": "blog_posts/2024-3-16/index.html#questions-to-answer",
    "title": "21st Century Energy Evolution: Trends & Transformations",
    "section": "",
    "text": "How has energy usage/consumption changed over time:\n\nHow has the overall usage changed over time?\nhow have the trends usage per sector changed between the 20th and 21st century?\nWhy are we seeing the trends we are?"
  },
  {
    "objectID": "blog_posts/2024-3-16/index.html#about-the-data",
    "href": "blog_posts/2024-3-16/index.html#about-the-data",
    "title": "21st Century Energy Evolution: Trends & Transformations",
    "section": "",
    "text": "All the data and definitions used in this infographic come from the U.S. Energy Information Administration website. The first dataset is annual primary energy consumption by U.S. sector in trillion British thermal units (Btu) from 1949 to 2022. Working with this data required me to combine two datasets that I downloaded separately and filter to the exact subset of data I was after for each plot. The second dataset is annual U.S. energy intensity from 1949 to 2022 in a variety of units. I also used the EIA glossary page to find variable descriptions on an as needed basis."
  },
  {
    "objectID": "blog_posts/2024-3-16/index.html#design-approach",
    "href": "blog_posts/2024-3-16/index.html#design-approach",
    "title": "21st Century Energy Evolution: Trends & Transformations",
    "section": "",
    "text": "graphic form: I found that the best way to display this data was with the fundamental chart types: area chart, line graph, and scatterplot. When looking at the data-to-viz website, there aren’t too many “creative” graphic forms to display trends over time. From the other options I found, including boxplots and ridgeline plots, the line and area plots made the most sense for this dataset, especially when including relevant linear regressions.\ntext (e.g. titles, captions, annotations, axis labels, axis ticks, alt text): For each of the figures below I was careful and considerate when choosing which text elements to include. Each of my figures include a title that gives the reader a quick overview of what data they are looking at. I chose not to include the take-home message in the title of the plot because I believe that information should be stored in the text of the infographic. Similarly with subtitles, only the energy intensity plot has a subtitle which describes what energy intensity is because it is not a commonly-known term. I generally would put the take-home message in the subtitle, but again that’s why there is accompanying text on the infographic. For captions, two of the graphs have captions with the data source included. The four middle graphs use the same dataset as cited in the very first graph and it felt redundant to include the same citations five times on the same infographic. Each figure as alt-text that describes the type of plot, the data being displayed, and the main message of the plot. For the two graphs that have captions the alt text is written into the code, and the four graphs that didn’t have captions in the infographic have captions and alt-text that I added in when I embedded the image into the site page. The full embedded inforgraphic also has alt-text. For axes properties, I made all the axes the same color as the background of the infographic for cohesion, and I used consitent scaling of the y-axis within each sector for easy comparison.\nthemes: I started each graph with theme_bw() and modified the elements from there including: removing legends, removing gridlines, removing panel border, and adjusting text. I decided to remove the gridlines from most of the plots because any plot showing trends doesn’t really need the gridlines, and with the linear regression plots that was just way too many lines to have on one small plot. I did decide to leave the major gridlines on the energy intensity plot because I feel like it’s nice to have an idea of roughly which time periods had the 15-10, 10-5, and &lt; 5, energy intensities. Since this infographic has a ton of plots, I used the large icons representing each of the sectors to help orient the reader before they get into the details of the graph. I feel like without reading much of the figure text, you can tell which sector is which and see there are clear changes in usage-trends for all four sectors.\ncolors: I was really stumped about which colors to use because in a perfect world the colors would help the reader identify the sector, but when I think of electricity I think of yellow and when I think of each sector, I basically get varying shades of grey. From personal choice, I hate looking at yellow on a grey background on a screen, it gives me an instant migraine. So I went with a dark blue background and used white icons and text blocks to highlight the plots, and used dark blue text throughout the infographic. I also kept the colors consistent throughout so each sector is always represented as the same color and I used the accent lines color (same hue darker shade of each color in the color palette) on the stacked area chart as the trendline color for the sector graphs.\ntypography: I used a serif/san-serif typeface pairing using Playfair Display and Nunito. I used Playfair Display for all of the chart and infographic titles to capture the more stylistic font on the visualization headlines. I utilized the san-serif font, Nunito, for its readability when I needed small font on graph axes labels, or when I had blocks of text in the infographic.\ngeneral design (e.g. group order, spacing, text orientation, data-ink ratio, creating a visual hierarchy, avoiding information overload): There is a ton of information on this infographic. I tried to combat the information overload by including these large, simple icons that help the reader break down the infographic into easily-digestible components. I also have my information organized in a beginning-middle-end format because I was trying to answer a question but I really took it in more of a storytelling direction. So I have the block at the top of the infographic that kind of sets the scene as the introduction with the data source, some context, and a super easy to digest graph. The middle then gets more creative and has more of the nitty-gritty data analysis, and at the end I have another graph with a text block that summarizes why we saw the trends that we did.\ncontextualizing your data: I added context to my data by weaving some of the key definitions and information about the data into the infographic so that readers don’t have to go to a new tab to look up the information they’d need to understand the figure. Hopefully, this makes this high level energy data more accessible to a wider audience.\ncentering your primary message: This is the part where I feel like I stumbled the most. I think I got caught up in the “storytelling” element and had a hard time really hitting the “each graph answers a clear individual question” part of the project. I tried to literally center the bulk of the data analysis and draw the reader in with the visual elements, but I’m hoping that story-line feel helps leave viewers with a lasting conclusion.\nconsidering accessibility (e.g. colorblind-friendly palettes / contrast, alt text) As I mentioned above, I was able to include alt-text for all of the figures on this site. And I used the Coolors website with the Let’s Get Colorblind Google Chrome extension to help me pick a color palette that was color-blind friendly.\napplying a DEI lens to your design (e.g. considering the people / communities / places represented in your data, consider how you frame your questions / issue) Using national level data that was collected by a government agency takes the people/communities element out of the project, but it does open up opportunities for bias. However by reporting my sources and letting the data tell me the story, I hope to have mitigated Equity/Integrity issues.\n\n\n\n\n\nShow the code\n#select relevant Data\nres20 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Residential Sector\" & year &lt; 2000)\n\nres21 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Residential Sector\" & year &gt;= 2000)\n# Perform linear regression\nres20lm &lt;- lm(Value ~ year, data = res20)\nres21lm &lt;- lm(Value ~ year, data = res21)\n#select relevant Data\ncomm20 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Commercial Sector\" & year &lt; 2000)\n\ncomm21 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Commercial Sector\" & year &gt;= 2000)\n# Perform linear regression\ncom20lm &lt;- lm(Value ~ year, data = comm20)\ncom21lm &lt;- lm(Value ~ year, data = comm21)\n#select relevant Data\nind20 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Industrial Sector\" & year &lt; 2000)\n\nind21 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Industrial Sector\" & year &gt;= 2000)\n# Perform linear regression\nind20lm &lt;- lm(Value ~ year, data = ind20)\nind21lm &lt;- lm(Value ~ year, data = ind21)\n#select relevant Data\ntrans20 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Transportation Sector\" & year &lt; 2000)\n\ntrans21 &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Transportation Sector\" & year &gt;= 2000)\n# Perform linear regression\ntr20 &lt;- lm(Value ~ year, data = trans20)\ntr21 &lt;- lm(Value ~ year, data = trans21)\n\n\n\n\n\n\n\nShow the code\np &lt;- sct_data %&gt;% \n  #Create a ggplot, fill based on description\n  ggplot(aes(x = year, y = Value, group = Description, fill = Description))+\n  #make it an area chart\n  geom_area(aes(color = Description), stat = \"identity\")+\n  #set the labels so the legend names aren't so long\n  scale_fill_manual(labels = c(\"Commercial\", \"Industrial\", \n                                 \"Residential\", \"Transportation\"), \n                    #add the color palette\n                    values = main_pal)+\n  scale_color_manual(values = accent_pal,\n                     guide = \"none\")+\n  #Add the chart labels/text\n  labs(title = \"Total Annual Energy Consumption by Sector from 1962-2022\",\n       x = \"Year\",\n       y = \"Total Energy Consumption (Trillion Btu)\",\n       caption = \"Source: Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\",\n       alt = \"The figure shows a stacked area chart for annual energy usage from 1962 to 2020 by sector for the Residential, Industrial, Commercial, and Transportation sectors in the United States. The graphs shows a gradual increase in the annual energy usage across all sectors until 2000, when the usage across all sectors begins to plateau. The total energy consumption of all sectors was just below 50,000 Trillion BTU in 1962 and plateaus around 75,000 BTU by 2022. The industrail and transportation setcors use the most energy with the commercial and residential sectors using slightly less overall.\")+\n  #expand to fill the whole space, and fix axis formatting\n  scale_y_continuous(expand = c(0, 0), labels = label_comma())+\n  scale_x_continuous(expand = c(0, 0))+\n  #Edit theme\n  theme_bw()+\n    theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #Remove Minor gridlines\n        panel.border = element_blank(), #Remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #Change the axis line width and color\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1),#Change the axis tick linewidth and color\n        legend.position = \"top\",  # Place legend at the top\n        legend.box = \"horizontal\",  # Make legend items horizontal\n        legend.direction = \"horizontal\",  # Set legend direction to horizontal\n        legend.title = element_blank(), #Remove legend title\n        plot.title = element_text(family=\"playfair\", #change title font\n                                  size=65,  #change title size\n                                  hjust = 0.5, #change title position\n                                  color = \"#264653\"), #change title color\n        plot.caption = element_text(family = \"nunito\", #Change axis font\n                              size = 25,#change caption font size\n                              face = \"bold\", #change caption font face\n                              color = \"#264653\",#change caption color\n                              hjust = 0.5), #center caption\n        axis.text = element_text(family = \"nunito\", #Change axis font\n                              size = 30,#change axis font size\n                              face = \"bold\", #change axis font face\n                              color = \"#264653\"),#change axis color\n        axis.title = element_text(family = \"playfair\", #Change axis title font\n                              size = 40, #Change axis title font size\n                              color = \"#264653\"), #Change axis title font color\n        legend.text = element_text(family = \"nunito\",  #Change legend font\n                                   size = 35, #Change legend font size\n                                   color = \"#264653\"), #Change legend font color\n        #Create space between the graph and the x axis\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")), \n        #Create space between the graph and the y axis\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n#ggsave(filename = \"infographic_plot1.png\", width = 16, height = 4)\n\n\n\n\n\n\n\n\nShow the code\n#------------------------------------------------------------------------------\n#                              Residential Sector                         \n#-------------------------------------------------------------------------------\n#Select Relevant Data\nPlot2.1a &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Residential Sector\") %&gt;% \n  filter(year &lt; 2000) %&gt;% \n#Create Plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#93B7BE\")+\n  #add the labels for the whole chart\n  labs(title = \"20th Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Residential Energy Consumption (Trillion Btu)\")+\n# Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = custom_breaks, #set custom breaks\n                     limits = custom_limits, #set custom limits\n                     labels = label_comma())+ #format axis labels\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#456B73\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #remove minor gridlines\n        panel.border = element_blank(), #remove border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #Set axis tick properties\n        legend.position = \"NA\",  # Remove Legend\n        #Set text properties including position, size, and color\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\" ),\n        #Expand Plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Select Relevant Data\nPlot2.1b &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Residential Sector\") %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n#Create Plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#93B7BE\")+\n  #add the labels for the whole chart\n  labs(title = \"21st Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Residential Energy Consumption (Trillion Btu)\")+\n  # Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = custom_breaks, \n                     limits = custom_limits, \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#456B73\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #remove minor gridlines\n        panel.border = element_blank(), #remove panel border,\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #Set axis tick properties\n        legend.position = \"NA\",  # Remove legend\n        #Set text properties including position, size, and color\n                plot.title = element_text(family=\"playfair\",\n                                          size=plot_title_size, \n                                          hjust = 0.5, \n                                          color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        #Expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Plot2.1a +Plot2.1b\n#ggsave(filename = \"infografphic_plot2.1.png\", width = 7, height = 4)\n\n#------------------------------------------------------------------------------\n#                              Industrial Sector                         \n#-------------------------------------------------------------------------------\n\n#Select Relevant Data\nPlot2.3a &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Industrial Sector\") %&gt;% \n  filter(year &lt; 2000) %&gt;% \n#Create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#B24C63\")+\n  #add the labels for the whole chart\n  labs(title = \"20th Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Industrial Energy Consumption (Trillion Btu)\")+\n#Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(25000,30000,35000), \n                     limits = c(20000,40000), \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#471F28\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #Remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1),#set axis tick properties\n        legend.position = \"NA\",  # Remove Legend\n        #Set plot text color, size on font\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        #expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Select relevant data\nPlot2.3b &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Industrial Sector\") %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  #Create Plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#B24C63\")+\n  #add the labels for the whole chart\n  labs(title = \"21st Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Industrial Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(25000,30000,35000), \n                     limits = c(20000,40000), \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#471F28\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(),#Remove minor gridlines\n        panel.border = element_blank(), #Remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #Set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set axis tick properties\n        legend.position = \"NA\",  # Remove legend\n        #Set plot text font, size color and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, color = \"#264653\"),\n        #Expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Plot2.3a + Plot2.3b\n#ggsave(filename = \"infografphic_plot2.3.png\", width = 7, height = 4)\n\n#------------------------------------------------------------------------------\n#                              Commercial Sector                         \n#-------------------------------------------------------------------------------\n#Select Relevant data\nPlot2.2a &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Commercial Sector\") %&gt;% \n  filter(year &lt; 2000) %&gt;% \n  #create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#8AB17D\")+\n  #add the labels for the whole chart\n  labs(title = \"20th Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Commercial Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = custom_breaks, \n                     limits = custom_limits, \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#3E5936\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set axis tick properties\n        legend.position = \"NA\",  # Remove legend\n        #set plot text font, color, size, and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, \n                                color = \"#264653\"),\n        #expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#select relevant Data\nPlot2.2b &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Commercial Sector\") %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  #create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#8AB17D\")+\n  #add the labels for the whole chart\n  labs(title = \"21st Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Commercial Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = custom_breaks, \n                     limits = custom_limits, \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#3E5936\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set asix line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set axis tick properties\n        legend.position = \"NA\",  # remove legend\n        #Set plot text font, color, size, and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, \n                                color = \"#264653\"),\n        #expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Plot2.2a + Plot2.2b\n#ggsave(filename = \"infografphic_plot2.2.png\", width = 7, height = 4)\n#------------------------------------------------------------------------------\n#                              Transportation sector                         \n#-------------------------------------------------------------------------------\n#Select relevant data\nPlot2.4a &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Transportation Sector\") %&gt;% \n  filter(year &lt; 2000) %&gt;% \n  #create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#F28F3B\")+\n  #add the labels for the whole chart\n  labs(title = \"20th Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Transportation Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(15000,20000,25000), \n                     limits = c(10000,30000), \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#864309\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #Remove minor gridlines\n        panel.border = element_blank(), #Remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set axis tick properties\n        legend.position = \"NA\",  # Remove legend\n        #Set plot text size, color, font and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 20,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, \n                                color = \"#264653\"),\n        #Expand plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Select relevant Data\nPlot2.4b &lt;- sct_data %&gt;% \n  filter(Description == \"Total Energy Consumed by the Transportation Sector\") %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  #Create plot\nggplot(aes(x = year, y = Value))+\n  geom_point(color = \"#F28F3B\")+\n  #add the labels for the whole chart\n  labs(title = \"21st Century Energy Use Trend\",\n       x = \"Year\",\n       y = \"Transportation Energy Consumption (Trillion Btu)\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(15000,20000,25000), \n                     limits = c(10000,30000), \n                     labels = label_comma())+\n  #add the trend line\n  geom_smooth(method = \"lm\", se = TRUE, show.legend = NA, color = \"#864309\")+\n  #set the theme\n  theme_bw()+\n  theme(panel.grid.major = element_blank(),  # Remove major gridlines\n        panel.grid.minor = element_blank(), #Remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #Set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #Set axis tick properties\n        legend.position = \"NA\",  # Place legend at the top\n        #Set plot text size, color, font, and position\n        plot.title = element_text(family=\"playfair\",\n                                  size=plot_title_size, \n                                  hjust = 0.25, \n                                  color = \"#264653\"),\n        axis.text = element_text(family = \"nunito\",\n                              size = 24,\n                              face = \"bold\", \n                              color = \"#264653\"),\n        axis.title = element_text(family = \"playfair\",\n                              size = axis_title_size, \n                              color = \"#264653\"),\n        strip.text = element_text(family = \"nunito\",\n                                size = 40, \n                                color = \"#264653\"),\n        #Expand Plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\n#Plot2.4a + Plot2.4b\n#ggsave(filename = \"infografphic_plot2.4.png\", width = 7, height = 4)\n\n\n\n\n\nSource:Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\n\n\n\n\n\nSource:Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\n\n\n\n\n\nSource:Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\n\n\n\n\n\nSource:Total Energy Annual Data—U.S. Energy Information Administration (EIA). (n.d.). Retrieved January 30, 2024, from https://www.eia.gov/totalenergy/data/annual/index.php\n\n\n\n\n\n\n\nShow the code\n#define subtitle text and set the wrapping properties\nsubtitle_text&lt;- str_wrap(\"Energy Intensity is a measure of how much energy is required to produce one unit of Gross Domestic Product. In this chart, energy intensity is measured in Thousand Btu per Chained (2017) Dollar\", width = 100)\ncaption_text &lt;- str_wrap(\"Source: U.S. Energy Information Administration—EIA - Independent Statistics and Analysis. (n.d.). Retrieved March 16, 2024, from https://www.eia.gov/totalenergy/data/browser/index.php?tbl=T01.07#/?f=A&start=1949&end=2020&charted=3\", width = 200)\n#Create the plot\ngraph &lt;- ggplot(data = intensity, aes(x = year, y = Value))+\n  geom_line(color = \"#78309C\", size = 3)+\n  geom_point(color = \"#D5B1E7\", size = 2)+\n  #Add plot labels including caption and alternative text\n    labs(title = \"Energy Intensity in the U.S. from 1949-2022\",\n         subtitle = subtitle_text,\n       x = \"Year\",\n       y = \"Energy Intensity \",\n       caption = caption_text,\n       alt= \"Figure shows a line chart of United States energy intensity from 1949 to 2022. There is a gradual decline in the energy intensity showing a 1949 energy intensity of around 13 Thousand Btu per Chained (2017) Dollar and a 2022 energy intensity of below 5 Thousand Btu per Chained (2017) Dollar, with the steady decrease happening between 1970 and 2022.\")+\n  #Adjust y axis properties\n  scale_y_continuous(expand = c(0, 0), \n                     breaks = c(5,10,15), \n                     limits = c(0,20))+\n  #Set theme properties\n  theme_bw()+\n  theme(\n        panel.grid.minor = element_blank(), #Remove minor gridlines\n        panel.border = element_blank(), #remove panel border\n        axis.line = element_line(color = \"#264653\", linewidth = 1), #Set axis line properties\n        axis.ticks = element_line(color = \"#264653\", linewidth = 1), #set the axis tick properties\n        plot.title = element_text(family=\"playfair\", #Set plot title font\n                                  size=50, #Set plot title size\n                                  hjust = 0.25, #Set plot title position\n                                  color = \"#264653\"),#Set plot title font color\n        plot.subtitle = element_text(family=\"nunito\", #Set plot subtitle font\n                                     size=30, #Set plot subtitle font size\n                                     color = \"#264653\", #Set plot subtitle font color\n                                     lineheight = 0.35), #Set plot subtitle linespacing\n        plot.caption = element_text(family = \"nunito\", #Change axis font\n                              size = 16,#change caption font size\n                              face = \"bold\", #change caption font face\n                              color = \"#264653\",#change caption color\n                              hjust = 0.5,\n                              lineheight = 0.35), #center caption\n        axis.text = element_text(family = \"nunito\", #Set axis text font\n                              size = 35, #Set axis text font size\n                              face = \"bold\", #Set axis text font face\n                              color = \"#264653\"), #Set axis text font color\n        axis.title = element_text(family = \"playfair\", #Set axis title font\n                              size = 40, #Set axis title font size\n                              color = \"#264653\"), #Set axis title font color\n        #Add space between the axes and plot\n        axis.title.x = element_text(margin = margin(4, 0, 0, 0, \"mm\")),\n        axis.title.y = element_text(margin = margin(0, 4, 0, 0, \"mm\")))\n\nggsave(filename = \"infografphic_plot3.png\", width = 7, height = 5)"
  },
  {
    "objectID": "blog_posts/2023-12-13/Blog_post_version.html",
    "href": "blog_posts/2023-12-13/Blog_post_version.html",
    "title": "Impact of the Thomas Fire on Santa Barbara County Air Quality and Land Cover Change",
    "section": "",
    "text": "Author: Heather Childers\nFor more information and a complete workflow with extra checks, consider checking out my github repositiory! Github Repo: https://github.com/hmchilders/ThomasFire_AQI"
  },
  {
    "objectID": "blog_posts/2023-12-13/Blog_post_version.html#about-this-notebook",
    "href": "blog_posts/2023-12-13/Blog_post_version.html#about-this-notebook",
    "title": "Impact of the Thomas Fire on Santa Barbara County Air Quality and Land Cover Change",
    "section": "About this Notebook:",
    "text": "About this Notebook:\nThe purpose of this exercise is to explore the impact of the 2017 Thomas fire on Santa Barbara County air quality and land cover. We’ll analyze the impact on air quality by plotting the daily and 5-day rolling average values of the air quality index for 2017 and 2018 to compare the AQI during the Thomas fire to the usual AQI for the area. We’ll explore the land cover change by using a false color image of Santa Barbara County and comparing it with the fire scar from the Thomas Fire.\nSome highlights of this analysis include: - Fetching data from an online repository - Data wrangling - Time Series analysis - Creating line plots with a legend - Creating a false color image - Visulaizing raster data"
  },
  {
    "objectID": "blog_posts/2023-12-13/Blog_post_version.html#about-the-data",
    "href": "blog_posts/2023-12-13/Blog_post_version.html#about-the-data",
    "title": "Impact of the Thomas Fire on Santa Barbara County Air Quality and Land Cover Change",
    "section": "About the data:",
    "text": "About the data:\nWe’ll be using the following Datasets for this analysis:\n\nAir Quality Index (AQI) data from the US Environmental Protection Agency to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County. The data can be accesses here: “AirData Website File Download Page.” EPA, Environmental Protection Agency, aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed 28 Nov. 2023.\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. The data can be accessed in the data folder of this repository. More info can be found here: “Microsoft Planetary Computer.” Planetary Computer, planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed 28 Nov. 2023.\nA shapefile of fire perimeters in California during 2017 The data can be accessed here: “California Fire Perimeters (All).” California State Geoportal, California Department of Forestry and Fire Protection, gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed 28 Nov. 2023.\n\nThese are the plots we’ll be trying to make: A timeseries analysis showing the daily average AQI and the 5-day rolling average from 2017 to 2018.\n\n\n\nAQIplot.png\n\n\nA false color image showing the fire scar from the Thomas fire in Santa Barbara County\n\n\n\nFireScar-2.png"
  },
  {
    "objectID": "blog_posts/2023-12-13/Blog_post_version.html#import-libraries",
    "href": "blog_posts/2023-12-13/Blog_post_version.html#import-libraries",
    "title": "Impact of the Thomas Fire on Santa Barbara County Air Quality and Land Cover Change",
    "section": "Import Libraries",
    "text": "Import Libraries\nFor this exercise, we’ll be using a variety of python packages. Please install these packages as needed before you import them.\n\n#Import the libraries and functions needed for this notebook\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\nfrom shapely import Point\nfrom shapely.geometry import box\nfrom rasterio.features import rasterize"
  },
  {
    "objectID": "blog_posts/2023-12-13/Blog_post_version.html#plot-the-aqi-data",
    "href": "blog_posts/2023-12-13/Blog_post_version.html#plot-the-aqi-data",
    "title": "Impact of the Thomas Fire on Santa Barbara County Air Quality and Land Cover Change",
    "section": "Plot the AQI Data",
    "text": "Plot the AQI Data\nUse the dataframe we cleaned and wrangled above to plot the Daily and 5_day average AQI data.\nDue to the way we cleaned and wrangled the data, we are able to use just the .plot() function with some simple customizations to create the desired visualization.\n\n# plot the data\naqi_sb.plot(use_index = True, \n        y= ['aqi', 'Five_day_average'],\n        title='Santa Barbara County Air Quality Index from 2017-2018',\n        xlabel='Year',\n        ylabel='Air Quality Index'\n        )\n\n&lt;Axes: title={'center': 'Santa Barbara County Air Quality Index from 2017-2018'}, xlabel='Year', ylabel='Air Quality Index'&gt;"
  },
  {
    "objectID": "blog_posts/2023-12-13/Blog_post_version.html#plot-the-land-cover-data",
    "href": "blog_posts/2023-12-13/Blog_post_version.html#plot-the-land-cover-data",
    "title": "Impact of the Thomas Fire on Santa Barbara County Air Quality and Land Cover Change",
    "section": "Plot the Land Cover Data",
    "text": "Plot the Land Cover Data\nNow that we have cleaned and wrangled all of the data, we are ready to make the final plot. It is important to remember that there are 4 main steps in making a more complicated visualizations with matplotlib.pyolot 1. Create the empty figure by defining the figure and axis 2. Add on the layers of data 3. Customize the non-data related features of the graph 4. Show the plot\n\n#----------STEP 1: Create Empty Figure---------------\n# Create the Figure and Axis\nfig, ax = plt.subplots()\n\n#---------STEP 2: Add in the data Layers-------------\n#Plot the Calfire Dataset\ncalfire.plot(ax=ax, #set the axis\n             color='red', #Change the fill color\n             alpha = 0.1, #Change the transparency of the layer\n             edgecolor='black') #Change the edge color\n#Plot the Landsat Data\nlandsat_false.plot.imshow(ax = ax, robust = True)\n\n#---------STEP 3: Customize the figure features-------\n#Add a figure title\nax.set_title('Fire Scar of the Thomas Fire in 2017 set over a Fasle Color Image of Santa Barbara, CA')\n#Turn the Axes off\nax.axis('off')\n#Add a figure legend\nfire_scar = mpatches.Patch(color='red', alpha = 0.6, label='Burned Area')\nax.legend(handles=[fire_scar])\n\n#--------STEP 4: Show the Plot-------------------------\nplt.show()"
  },
  {
    "objectID": "blog_posts/2023-12-14/Stats.html",
    "href": "blog_posts/2023-12-14/Stats.html",
    "title": "Impact of Land Use Change on Water Storage in the Yolo County Area",
    "section": "",
    "text": "After high school, I knew I wanted to leave Santa Barbara and gain some independence in a new town. When applying to colleges I fell in love with the small town of Davis, California. While at UC Davis, I got my undergraduate degree in Biological and Agricultural Engineering. One of the things I enjoyed about studying agriculture is how interdisciplinary the field is, within agriculture you can specialize in genetic engineering, biology, chemistry, climate, sustainability, etc.\nAs populations continue to grow, pressure for the agricultural industry to keep up with increasing food demands while reducing resource consumption grows with it. Since Davis is typically known to the students as “Ag Town”, I thought it would be interesting to see how the land usage of cultivated crops has changed over time. Similarly, I hypothesized that there would be a negative correlation between area of cultivated crop land, and water storage.\n\n\n\n[Figure 1] Land Cover for the Yolo County Area in Northern California. The majority of the land cover in this area is cultivated crop land (shown in brown on this graph). This annual land cover data was used as the basis for my analysis. Source: National Land Cover Database 2021 - Landcover & Imperviousness (NLCD2021). Dewitz, J., 2023, National Land Cover Database (NLCD) 2021 Products: U.S. Geological Survey data release. https://www.mrlc.gov/data/nlcd-2021-land-cover-conus\n\n\nThis analysis aims to see if there is a correlation between the area of cultivated crops and water storage. This analysis will involve nonlinear relationships in linear regression models, multiple linear regressions, evaluation of omitted variables bias, hypothesis testing, and time series decomposition.\nFor this analysis, my null and alternative hypotheses are as follows:\nH0: There is no correlation between agricultural land use change and water storage in the Yolo County area.\nHA: There is a correlation between agricultural land use change and water storage in the Yolo County area.\n\n\n\nThere were four data sources used for this analysis.\n\n\nThe Multi-Resolution Land Characteristics Consortium has put together a National Land Cover Database for the United States. The land cover database is updated every 2-3 years with a pre-processed land cover raster. The land cover and land cover change data has a resolution of 30m with a 16-class legend. For this analysis, I accessed the Continental United States data for all years. The all-years dataset includes the years 2003, 2006, 2008, 2011, 2013, 2016, 2019, and 2021. The data download can be found here:\nMulti-Resolution Land Characteristics Consortium. (n.d.). NLCD land cover (CONUS) all years. NLCD Land Cover (CONUS) All Years. https://www.mrlc.gov/data/nlcd-land-cover-conus-all-years\n\n\n\nThe GRACE twin Satellites project is run by NASA, and works to estimate changes in water storage. These satellites work by measuring small changes in the Earth’s gravity due to changes in weight. The changes in gravity are then processed by JPL to get a Water Equivalent Thickness Anomaly. The Water equivalent thickness describes the total water in the soil as a layer of water sitting on top of the soil of a given thickness in centimeters. The anomalies are calculated using the measurements from Jan. 2004- Dec. 2009 as the baseline. The data can be accessed here:\nNASA. (n.d.). Grace. NASA. https://grace.jpl.nasa.gov/data-analysis-tool/#b=ESRI_World_Imagery&l=OSMCoastlines(1)&vm=2D&ve=-138.0121190906666,30.247546877703407,-95.6596042881563,54.8573720600865&pl=false&pb=false&tr=false&d=2003-06-15&tlr=months\n\n\n\nThe California Department of water Resources has monthly precipitation data for individual stations within California. For this anlysis I used the Woodland station operated by the National Weather Service. I chose this station because I recognized Woodland as a nearby town to Davis where many students go shopping. I used this stations precipitation data as a proxy for rainfall in the Yolo County Area. The data can be found here:\nCalifornia, S. of. (n.d.). CDEC Web Applications. California Data Exchange Center. https://cdec.water.ca.gov/dynamicapp/QueryMonthly?s=WDL&end=2023-12&span=5years\n\n\n\nIn order to crop the continental US data from NCLD to my area of interest, I downloaded the shapefiles for California and all of the counties. These shapfiles are available for download via the California Open Data Portal. The data is available for download here:\nCA geographic boundaries. CA Geographic Boundaries | CA Open Data. (n.d.). https://lab.data.ca.gov/dataset/ca-geographic-boundaries\n\n\n\n\nMy analysis involved three main steps: data exploration and cleaning, data visualization, and linear regression modeling.\n\n\nIn order to prepare the NLCD data for my experiment, I needed to load in the data, crop it to my area of interest (Yolo County) using the CA county shapefiles, and then create a for loop that runs zonal statistics for each available dataset. This allowed me to create a dataframe that showed the total area of cultivated cropland from 2003-2021.\nI was easily able to download the water equivalent thickness anomaly data from the GRACE interactive site, and specify the area of interest before downloading the data. I was able to use group_by() and summarize() with some other simple manipulations to plot the annual average water equivalent thickness anomaly in centimeters which I simplified to the variable name mean_water. After downloading the data, I performed an STL decomposition so I could see any trends in the water storage data.\nSimilarly, I was easily able to download the monthly precipitation data in 5-year increments from the California Department of Water Resources website. I used to rbind() function to combine three datasets into one dataframe that included data from 2003-2019. From this dataframe I was able to calculate the annual average precipitation in centimeters. I then performed an STL decomposition so I could see any trends in the precipitation data.\n\n\n\nNow that I had a few cleaned and wrangled datasets, I was able to visualize the data before performing linear regressions. The two visualizations I used are shown below in Figures 2 and 3.\n\n\n\n[Figure 2] The left figure shows the change in the area of cultivated croplands from 2003-2021 in square meters. The right figure shows the STL decomposition of the annual average water equivalent thickness anomaly(cm) from 2002-2019.\n\n\nThe most notable result from this graph is the clear negative correlation between the area of cultivated crops and the trend of water storage in the STL decomposition.\n\n\n\n[Figure 3] This figure shows the comparison of two STL decompositions. The left three graphs show the STL decomposition for the Annual Average Precipitation(centimeters) measured by the Woodland Weather Station. The right three graphs show the STL decomposition of the annual average water equivalent thickness anomaly(cm) from 2002-2019.\n\n\nWhat I found most interesting about these graphs is that the remainders of these two decompositions seem more similar that the trends.\nThese two visualizations provided some intuition that the cultivated crop land is responsible for more of the overall trends seen in the water storage but the annual rainfall accounts for more of the noise/short term fluxes in water storage.\n\n\n\nIn order to try to find the best estimates from my linear regression models, I performed multiple analyses.\nThe first linear regression was a simple linear regression showing the correlation between the water storage and cultivated crop area.\nThe next set of linear regressions used polynomials for the cultivated crop variable. I ran two experiments looking at a quadratic and cubic equation for cultivated cropland.\nIn order to test for omitted variables bias, I tried to improve my model by running two multiple linear regression models that used changes in developed areas and annual average precipitation as the additional variable in the multiple linear regression model.\n\n\n\n\nAfter running a variety of linear regressions, I found that the model with the best fit for my data was a multiple linear regression model with cultivated crop area and annual average precipitation as the variables. Figure 4 shows a summary of the multiple linear regression output with the best adjusted R-squared value and best p-values for my coefficient estimates.\n\n\n\n[Figure 4] Summary output of the multiple linear regression model of cultivated crop area and annual average precipitation on water storage.\n\n\nIt is worth noting that the Adjusted R-squared value is 0.4972, which implies that 49% of the variation in my model could be explained by just cultivated cropland use change and annual precipitation with a penalty for a more complex model.\nBased on the results of my summary above, I was not able to reject the null hypothesis at a significance level of 0.5. While I could not reject the null hypothesis, these p-values are not so high that they imply there is no relationship between these variables. The fact that I was able to increase my adjusted R-squared value by adding in annual precipitation implies that there was omitted variables bias in the simple linear regression. This makes sense because the total water storage is likely affected by many factors that were not accounted for in this short analysis.\n\n\n\nIf I had more time to clean more data and run more analyses, I would try to add more variables to this analysis and find out which of a larger set of variables has the largest impact on the water storage in the Yolo County Area.\n\n\n\nThe code for all the data cleaning, visualizations, and linear regressions is available on my Github here.\n\n\n\nCA geographic boundaries. CA Geographic Boundaries | CA Open Data. (n.d.). https://lab.data.ca.gov/dataset/ca-geographic-boundaries\nCalifornia, S. of. (n.d.). CDEC Web Applications. California Data Exchange Center. https://cdec.water.ca.gov/dynamicapp/QueryMonthly?s=WDL&end=2023-12&span=5years\nMulti-Resolution Land Characteristics Consortium. (n.d.). NLCD land cover (CONUS) all years. NLCD Land Cover (CONUS) All Years . https://www.mrlc.gov/data/nlcd-land-cover-conus-all-years\nNASA. (2018, February 22). Grace. NASA. https://grace.jpl.nasa.gov/mission/grace/\nNASA. (2023, March 7). Overview - monthly mass grids. NASA. https://grace.jpl.nasa.gov/data/monthly-mass-grids/\nNASA. (n.d.). Grace. NASA. https://grace.jpl.nasa.gov/data-analysis-tool/#b=ESRI_World_Imagery&l=OSMCoastlines(1)&vm=2D&ve=-138.0121190906666,30.247546877703407,-95.6596042881563,54.8573720600865&pl=false&pb=false&tr=false&d=2003-06-15&tlr=months\nStonestrom, D. A., Scanlon, B. R., & Zhang, L. (2009, March 3). Introduction to special section on impacts of land use change on water ... Introduction to special section on Impacts of Land Use Change on Water Resources. https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2009WR007937"
  },
  {
    "objectID": "blog_posts/2023-12-14/Stats.html#motivation",
    "href": "blog_posts/2023-12-14/Stats.html#motivation",
    "title": "Impact of Land Use Change on Water Storage in the Yolo County Area",
    "section": "",
    "text": "After high school, I knew I wanted to leave Santa Barbara and gain some independence in a new town. When applying to colleges I fell in love with the small town of Davis, California. While at UC Davis, I got my undergraduate degree in Biological and Agricultural Engineering. One of the things I enjoyed about studying agriculture is how interdisciplinary the field is, within agriculture you can specialize in genetic engineering, biology, chemistry, climate, sustainability, etc.\nAs populations continue to grow, pressure for the agricultural industry to keep up with increasing food demands while reducing resource consumption grows with it. Since Davis is typically known to the students as “Ag Town”, I thought it would be interesting to see how the land usage of cultivated crops has changed over time. Similarly, I hypothesized that there would be a negative correlation between area of cultivated crop land, and water storage.\n\n\n\n[Figure 1] Land Cover for the Yolo County Area in Northern California. The majority of the land cover in this area is cultivated crop land (shown in brown on this graph). This annual land cover data was used as the basis for my analysis. Source: National Land Cover Database 2021 - Landcover & Imperviousness (NLCD2021). Dewitz, J., 2023, National Land Cover Database (NLCD) 2021 Products: U.S. Geological Survey data release. https://www.mrlc.gov/data/nlcd-2021-land-cover-conus\n\n\nThis analysis aims to see if there is a correlation between the area of cultivated crops and water storage. This analysis will involve nonlinear relationships in linear regression models, multiple linear regressions, evaluation of omitted variables bias, hypothesis testing, and time series decomposition.\nFor this analysis, my null and alternative hypotheses are as follows:\nH0: There is no correlation between agricultural land use change and water storage in the Yolo County area.\nHA: There is a correlation between agricultural land use change and water storage in the Yolo County area."
  },
  {
    "objectID": "blog_posts/2023-12-14/Stats.html#data",
    "href": "blog_posts/2023-12-14/Stats.html#data",
    "title": "Impact of Land Use Change on Water Storage in the Yolo County Area",
    "section": "",
    "text": "There were four data sources used for this analysis.\n\n\nThe Multi-Resolution Land Characteristics Consortium has put together a National Land Cover Database for the United States. The land cover database is updated every 2-3 years with a pre-processed land cover raster. The land cover and land cover change data has a resolution of 30m with a 16-class legend. For this analysis, I accessed the Continental United States data for all years. The all-years dataset includes the years 2003, 2006, 2008, 2011, 2013, 2016, 2019, and 2021. The data download can be found here:\nMulti-Resolution Land Characteristics Consortium. (n.d.). NLCD land cover (CONUS) all years. NLCD Land Cover (CONUS) All Years. https://www.mrlc.gov/data/nlcd-land-cover-conus-all-years\n\n\n\nThe GRACE twin Satellites project is run by NASA, and works to estimate changes in water storage. These satellites work by measuring small changes in the Earth’s gravity due to changes in weight. The changes in gravity are then processed by JPL to get a Water Equivalent Thickness Anomaly. The Water equivalent thickness describes the total water in the soil as a layer of water sitting on top of the soil of a given thickness in centimeters. The anomalies are calculated using the measurements from Jan. 2004- Dec. 2009 as the baseline. The data can be accessed here:\nNASA. (n.d.). Grace. NASA. https://grace.jpl.nasa.gov/data-analysis-tool/#b=ESRI_World_Imagery&l=OSMCoastlines(1)&vm=2D&ve=-138.0121190906666,30.247546877703407,-95.6596042881563,54.8573720600865&pl=false&pb=false&tr=false&d=2003-06-15&tlr=months\n\n\n\nThe California Department of water Resources has monthly precipitation data for individual stations within California. For this anlysis I used the Woodland station operated by the National Weather Service. I chose this station because I recognized Woodland as a nearby town to Davis where many students go shopping. I used this stations precipitation data as a proxy for rainfall in the Yolo County Area. The data can be found here:\nCalifornia, S. of. (n.d.). CDEC Web Applications. California Data Exchange Center. https://cdec.water.ca.gov/dynamicapp/QueryMonthly?s=WDL&end=2023-12&span=5years\n\n\n\nIn order to crop the continental US data from NCLD to my area of interest, I downloaded the shapefiles for California and all of the counties. These shapfiles are available for download via the California Open Data Portal. The data is available for download here:\nCA geographic boundaries. CA Geographic Boundaries | CA Open Data. (n.d.). https://lab.data.ca.gov/dataset/ca-geographic-boundaries"
  },
  {
    "objectID": "blog_posts/2023-12-14/Stats.html#analysis",
    "href": "blog_posts/2023-12-14/Stats.html#analysis",
    "title": "Impact of Land Use Change on Water Storage in the Yolo County Area",
    "section": "",
    "text": "My analysis involved three main steps: data exploration and cleaning, data visualization, and linear regression modeling.\n\n\nIn order to prepare the NLCD data for my experiment, I needed to load in the data, crop it to my area of interest (Yolo County) using the CA county shapefiles, and then create a for loop that runs zonal statistics for each available dataset. This allowed me to create a dataframe that showed the total area of cultivated cropland from 2003-2021.\nI was easily able to download the water equivalent thickness anomaly data from the GRACE interactive site, and specify the area of interest before downloading the data. I was able to use group_by() and summarize() with some other simple manipulations to plot the annual average water equivalent thickness anomaly in centimeters which I simplified to the variable name mean_water. After downloading the data, I performed an STL decomposition so I could see any trends in the water storage data.\nSimilarly, I was easily able to download the monthly precipitation data in 5-year increments from the California Department of Water Resources website. I used to rbind() function to combine three datasets into one dataframe that included data from 2003-2019. From this dataframe I was able to calculate the annual average precipitation in centimeters. I then performed an STL decomposition so I could see any trends in the precipitation data.\n\n\n\nNow that I had a few cleaned and wrangled datasets, I was able to visualize the data before performing linear regressions. The two visualizations I used are shown below in Figures 2 and 3.\n\n\n\n[Figure 2] The left figure shows the change in the area of cultivated croplands from 2003-2021 in square meters. The right figure shows the STL decomposition of the annual average water equivalent thickness anomaly(cm) from 2002-2019.\n\n\nThe most notable result from this graph is the clear negative correlation between the area of cultivated crops and the trend of water storage in the STL decomposition.\n\n\n\n[Figure 3] This figure shows the comparison of two STL decompositions. The left three graphs show the STL decomposition for the Annual Average Precipitation(centimeters) measured by the Woodland Weather Station. The right three graphs show the STL decomposition of the annual average water equivalent thickness anomaly(cm) from 2002-2019.\n\n\nWhat I found most interesting about these graphs is that the remainders of these two decompositions seem more similar that the trends.\nThese two visualizations provided some intuition that the cultivated crop land is responsible for more of the overall trends seen in the water storage but the annual rainfall accounts for more of the noise/short term fluxes in water storage.\n\n\n\nIn order to try to find the best estimates from my linear regression models, I performed multiple analyses.\nThe first linear regression was a simple linear regression showing the correlation between the water storage and cultivated crop area.\nThe next set of linear regressions used polynomials for the cultivated crop variable. I ran two experiments looking at a quadratic and cubic equation for cultivated cropland.\nIn order to test for omitted variables bias, I tried to improve my model by running two multiple linear regression models that used changes in developed areas and annual average precipitation as the additional variable in the multiple linear regression model."
  },
  {
    "objectID": "blog_posts/2023-12-14/Stats.html#results",
    "href": "blog_posts/2023-12-14/Stats.html#results",
    "title": "Impact of Land Use Change on Water Storage in the Yolo County Area",
    "section": "",
    "text": "After running a variety of linear regressions, I found that the model with the best fit for my data was a multiple linear regression model with cultivated crop area and annual average precipitation as the variables. Figure 4 shows a summary of the multiple linear regression output with the best adjusted R-squared value and best p-values for my coefficient estimates.\n\n\n\n[Figure 4] Summary output of the multiple linear regression model of cultivated crop area and annual average precipitation on water storage.\n\n\nIt is worth noting that the Adjusted R-squared value is 0.4972, which implies that 49% of the variation in my model could be explained by just cultivated cropland use change and annual precipitation with a penalty for a more complex model.\nBased on the results of my summary above, I was not able to reject the null hypothesis at a significance level of 0.5. While I could not reject the null hypothesis, these p-values are not so high that they imply there is no relationship between these variables. The fact that I was able to increase my adjusted R-squared value by adding in annual precipitation implies that there was omitted variables bias in the simple linear regression. This makes sense because the total water storage is likely affected by many factors that were not accounted for in this short analysis."
  },
  {
    "objectID": "blog_posts/2023-12-14/Stats.html#future-work",
    "href": "blog_posts/2023-12-14/Stats.html#future-work",
    "title": "Impact of Land Use Change on Water Storage in the Yolo County Area",
    "section": "",
    "text": "If I had more time to clean more data and run more analyses, I would try to add more variables to this analysis and find out which of a larger set of variables has the largest impact on the water storage in the Yolo County Area."
  },
  {
    "objectID": "blog_posts/2023-12-14/Stats.html#code-availability",
    "href": "blog_posts/2023-12-14/Stats.html#code-availability",
    "title": "Impact of Land Use Change on Water Storage in the Yolo County Area",
    "section": "",
    "text": "The code for all the data cleaning, visualizations, and linear regressions is available on my Github here."
  },
  {
    "objectID": "blog_posts/2023-12-14/Stats.html#citations",
    "href": "blog_posts/2023-12-14/Stats.html#citations",
    "title": "Impact of Land Use Change on Water Storage in the Yolo County Area",
    "section": "",
    "text": "CA geographic boundaries. CA Geographic Boundaries | CA Open Data. (n.d.). https://lab.data.ca.gov/dataset/ca-geographic-boundaries\nCalifornia, S. of. (n.d.). CDEC Web Applications. California Data Exchange Center. https://cdec.water.ca.gov/dynamicapp/QueryMonthly?s=WDL&end=2023-12&span=5years\nMulti-Resolution Land Characteristics Consortium. (n.d.). NLCD land cover (CONUS) all years. NLCD Land Cover (CONUS) All Years . https://www.mrlc.gov/data/nlcd-land-cover-conus-all-years\nNASA. (2018, February 22). Grace. NASA. https://grace.jpl.nasa.gov/mission/grace/\nNASA. (2023, March 7). Overview - monthly mass grids. NASA. https://grace.jpl.nasa.gov/data/monthly-mass-grids/\nNASA. (n.d.). Grace. NASA. https://grace.jpl.nasa.gov/data-analysis-tool/#b=ESRI_World_Imagery&l=OSMCoastlines(1)&vm=2D&ve=-138.0121190906666,30.247546877703407,-95.6596042881563,54.8573720600865&pl=false&pb=false&tr=false&d=2003-06-15&tlr=months\nStonestrom, D. A., Scanlon, B. R., & Zhang, L. (2009, March 3). Introduction to special section on impacts of land use change on water ... Introduction to special section on Impacts of Land Use Change on Water Resources. https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2009WR007937"
  },
  {
    "objectID": "blog_posts/2023-12-06/STAC (1).html",
    "href": "blog_posts/2023-12-06/STAC (1).html",
    "title": "Using the Microsoft Planetary Computer",
    "section": "",
    "text": "#Import Libraries\nimport numpy as np\nimport geopandas as gpd\nimport rioxarray as rioxr\nimport matplotlib.pyplot as plt\n\nfrom shapely.geometry import Polygon\n\n# used to access STAC catalogs\nfrom pystac_client import Client\n# used to sign items from the MPC STAC catalog\nimport planetary_computer\n\n# ----- other libraries for nice ouputs\nfrom IPython.display import Image"
  },
  {
    "objectID": "blog_posts/2023-12-06/STAC (1).html#access",
    "href": "blog_posts/2023-12-06/STAC (1).html#access",
    "title": "Using the Microsoft Planetary Computer",
    "section": "Access",
    "text": "Access\nWe use the Client function from the pystac_client package to access the catalog:\n\n# access catalog\ncatalog = Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\nThe modifier parameter is needed to access the data in the MPC catalog."
  },
  {
    "objectID": "blog_posts/2023-12-06/STAC (1).html#exploration",
    "href": "blog_posts/2023-12-06/STAC (1).html#exploration",
    "title": "Using the Microsoft Planetary Computer",
    "section": "Exploration",
    "text": "Exploration\nLet’s check out some of the catalog’s metadata:\n\n# metadata from the catalog\n#print('ID:', catalog.id)\nprint('Title:', catalog.title)\nprint('Description:', catalog.description)\n\nTitle: Microsoft Planetary Computer STAC API\nDescription: Searchable spatiotemporal metadata describing Earth science datasets hosted by the Microsoft Planetary Computer\n\n\nWe can access its collections by using the get_collections() method:\n\ncatalog.get_collections()\n\n&lt;generator object Client.get_collections at 0x72f428c48f20&gt;\n\n\nNotice the output of get_collections() is a generator. This is a special kind of lazy object in Python over which you can loop over like a list.\nUnlike a list, the items in a generator do not exist in memory until you explicitely iterate over them or convert them to a list. Let’s try getting the collections from the catalog again:\n\n# get collections and print their names\ncollections = list(catalog.get_collections())\n\nprint('Number of collections:', len(collections))\nprint(\"Collections IDs:\")\nfor collection in collections:\n    print('-', collection.id)\n\nNumber of collections: 122\nCollections IDs:\n- daymet-annual-pr\n- daymet-daily-hi\n- 3dep-seamless\n- 3dep-lidar-dsm\n- fia\n- sentinel-1-rtc\n- gridmet\n- daymet-annual-na\n- daymet-monthly-na\n- daymet-annual-hi\n- daymet-monthly-hi\n- daymet-monthly-pr\n- gnatsgo-tables\n- hgb\n- cop-dem-glo-30\n- cop-dem-glo-90\n- goes-cmi\n- terraclimate\n- nasa-nex-gddp-cmip6\n- gpm-imerg-hhr\n- gnatsgo-rasters\n- 3dep-lidar-hag\n- 3dep-lidar-intensity\n- 3dep-lidar-pointsourceid\n- mtbs\n- noaa-c-cap\n- 3dep-lidar-copc\n- modis-64A1-061\n- alos-fnf-mosaic\n- 3dep-lidar-returns\n- mobi\n- landsat-c2-l2\n- era5-pds\n- chloris-biomass\n- kaza-hydroforecast\n- planet-nicfi-analytic\n- modis-17A2H-061\n- modis-11A2-061\n- daymet-daily-pr\n- 3dep-lidar-dtm-native\n- 3dep-lidar-classification\n- 3dep-lidar-dtm\n- gap\n- modis-17A2HGF-061\n- planet-nicfi-visual\n- gbif\n- modis-17A3HGF-061\n- modis-09A1-061\n- alos-dem\n- alos-palsar-mosaic\n- deltares-water-availability\n- modis-16A3GF-061\n- modis-21A2-061\n- us-census\n- jrc-gsw\n- deltares-floods\n- modis-43A4-061\n- modis-09Q1-061\n- modis-14A1-061\n- hrea\n- modis-13Q1-061\n- modis-14A2-061\n- sentinel-2-l2a\n- modis-15A2H-061\n- modis-11A1-061\n- modis-15A3H-061\n- modis-13A1-061\n- daymet-daily-na\n- nrcan-landcover\n- modis-10A2-061\n- ecmwf-forecast\n- noaa-mrms-qpe-24h-pass2\n- sentinel-1-grd\n- nasadem\n- io-lulc\n- landsat-c2-l1\n- drcog-lulc\n- chesapeake-lc-7\n- chesapeake-lc-13\n- chesapeake-lu\n- noaa-mrms-qpe-1h-pass1\n- noaa-mrms-qpe-1h-pass2\n- noaa-nclimgrid-monthly\n- goes-glm\n- usda-cdl\n- eclipse\n- esa-cci-lc\n- esa-cci-lc-netcdf\n- fws-nwi\n- usgs-lcmap-conus-v13\n- usgs-lcmap-hawaii-v10\n- noaa-climate-normals-tabular\n- noaa-climate-normals-netcdf\n- noaa-climate-normals-gridded\n- aster-l1t\n- cil-gdpcir-cc-by-sa\n- io-lulc-9-class\n- io-biodiversity\n- naip\n- noaa-cdr-sea-surface-temperature-whoi\n- noaa-cdr-ocean-heat-content\n- cil-gdpcir-cc0\n- cil-gdpcir-cc-by\n- noaa-cdr-sea-surface-temperature-whoi-netcdf\n- noaa-cdr-sea-surface-temperature-optimum-interpolation\n- modis-10A1-061\n- sentinel-5p-l2-netcdf\n- sentinel-3-olci-wfr-l2-netcdf\n- noaa-cdr-ocean-heat-content-netcdf\n- sentinel-3-synergy-aod-l2-netcdf\n- sentinel-3-synergy-v10-l2-netcdf\n- sentinel-3-olci-lfr-l2-netcdf\n- sentinel-3-sral-lan-l2-netcdf\n- sentinel-3-slstr-lst-l2-netcdf\n- sentinel-3-slstr-wst-l2-netcdf\n- sentinel-3-sral-wat-l2-netcdf\n- ms-buildings\n- sentinel-3-slstr-frp-l2-netcdf\n- sentinel-3-synergy-syn-l2-netcdf\n- sentinel-3-synergy-vgp-l2-netcdf\n- sentinel-3-synergy-vg1-l2-netcdf\n- esa-worldcover"
  },
  {
    "objectID": "blog_posts/2023-12-06/STAC (1).html#exercise",
    "href": "blog_posts/2023-12-06/STAC (1).html#exercise",
    "title": "Using the Microsoft Planetary Computer",
    "section": "Exercise:",
    "text": "Exercise:\nThe ‘cop-dem-glo-90’ collection contains the Copernicus DEM at 90m resolution (the data we previously used for the Grand Canyon).\n\nReuse the bbox for Santa Barbara to look for items in this collection.\nGet the first item in the search and check its assets.\nCheck the item’s rendered preview asset by clicking on it’s URL.\nOpen the item’s data using rioxarray.\n\n\n#Reuse the bbox for Santa Barbara to look for items in this collection.\n#Catalog Search:\nsearch_1 = catalog.search(\n    collections = ['cop-dem-glo-90'],\n    intersects = bbox)\n\n\n#Check the collection\nitems_1 = search_1.item_collection()\n\n\n#Get the first item in the search.\nC_DEM = items_1[0]\n#check its assets\nC_DEM.assets\n\n{'data': &lt;Asset href=https://elevationeuwest.blob.core.windows.net/copernicus-dem/COP90_hh/Copernicus_DSM_COG_30_N34_00_W120_00_DEM.tif?st=2023-12-05T20%3A43%3A51Z&se=2023-12-13T20%3A43%3A52Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-12-06T20%3A43%3A50Z&ske=2023-12-13T20%3A43%3A50Z&sks=b&skv=2021-06-08&sig=0yTIfzgRp2a89msKT6w3F2XfJHiO%2BvCfPi04Zwi6vJQ%3D&gt;,\n 'tilejson': &lt;Asset href=https://planetarycomputer.microsoft.com/api/data/v1/item/tilejson.json?collection=cop-dem-glo-90&item=Copernicus_DSM_COG_30_N34_00_W120_00_DEM&assets=data&colormap_name=terrain&rescale=-1000%2C4000&format=png&gt;,\n 'rendered_preview': &lt;Asset href=https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=cop-dem-glo-90&item=Copernicus_DSM_COG_30_N34_00_W120_00_DEM&assets=data&colormap_name=terrain&rescale=-1000%2C4000&format=png&gt;}\n\n\n\nfor key in C_DEM.assets.keys():\n    print(key, '--', C_DEM.assets[key].title)\n\ndata -- N34_00_W120_00\ntilejson -- TileJSON with default rendering\nrendered_preview -- Rendered preview\n\n\n\n# Check the item’s rendered preview asset by clicking on it’s URL.\nImage(url=C_DEM.assets['rendered_preview'].href, width=500)\n\n\n\n\n\n#Open the item’s data using rioxarray.\nDEM = rioxr.open_rasterio(C_DEM.assets['data'].href)\nDEM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 1200, x: 1200)&gt;\n[1440000 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -120.0 -120.0 -120.0 ... -119.0 -119.0 -119.0\n  * y            (y) float64 35.0 35.0 35.0 35.0 35.0 ... 34.0 34.0 34.0 34.0\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Point\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 1200x: 1200...[1440000 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-120.0 -120.0 ... -119.0 -119.0array([-120.      , -119.999167, -119.998333, ..., -119.0025  , -119.001667,\n       -119.000833])y(y)float6435.0 35.0 35.0 ... 34.0 34.0 34.0array([35.      , 34.999167, 34.998333, ..., 34.0025  , 34.001667, 34.000833])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-120.00041666666667 0.0008333333333333334 0.0 35.000416666666666 0.0 -0.0008333333333333334array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([             -120.0, -119.99916666666667, -119.99833333333333,\n                 -119.9975, -119.99666666666667, -119.99583333333334,\n                  -119.995, -119.99416666666667, -119.99333333333334,\n                 -119.9925,\n       ...\n       -119.00833333333334,           -119.0075, -119.00666666666666,\n       -119.00583333333333,            -119.005, -119.00416666666666,\n       -119.00333333333333,           -119.0025, -119.00166666666667,\n       -119.00083333333333],\n      dtype='float64', name='x', length=1200))yPandasIndexPandasIndex(Index([              35.0,  34.99916666666667, 34.998333333333335,\n                  34.9975,  34.99666666666667,  34.99583333333333,\n                   34.995, 34.994166666666665,  34.99333333333333,\n                  34.9925,\n       ...\n        34.00833333333333,            34.0075,  34.00666666666667,\n       34.005833333333335,             34.005,  34.00416666666667,\n        34.00333333333333,            34.0025, 34.001666666666665,\n        34.00083333333333],\n      dtype='float64', name='y', length=1200))Attributes: (3)AREA_OR_POINT :Pointscale_factor :1.0add_offset :0.0"
  },
  {
    "objectID": "blog_posts/2023-11-06_practice_blog_post/index.html",
    "href": "blog_posts/2023-11-06_practice_blog_post/index.html",
    "title": "A Comparison of Common Indices Used to Measure Poverty and Inequality",
    "section": "",
    "text": "As an emerging environmental data scientist, I am always looking to learn more about data visualizations that allow technical and non-technical groups to communicate effectively. In the pursuit of equality within countries, and globally, it is important to have a standardized way of comparing wealth disparities. The goal of this paper is to summarize some of the most commonly used indices for measuring poverty and inequalities for a non-technical audience while providing an accurate account of the benefits and drawbacks of each measurement. A description of some less commonly used indices that provide an alternative approach and opportunities for future work have been included as well.\n\n\n\nIn many countries, the easiest and most commonly used indices of success and equality use income data as the only metric. This is partly because monetary data, including average household income and GDP per capita, is easy to collect and paints a picture of a country’s success without being fully transparent about the severity of inequalities. The problem with many of these commonly used poverty indices is that they aren’t always distribution sensitive, meaning incomes slightly below the poverty line are weighted equally compared to an income that is grossly below the poverty line. The benefit of using these indices that aren’t the most transparent about the income inequality is that the units on these indices are generally easy for the average person to interpret. The following poverty indices use income and population data to calculate an index that can be compared between smaller and larger groups regardless of currency.\n\n\nThe poverty gap index provides as estimate of how far below the poverty line people within a population fall. This approach begins by defining the poverty line for each group of interest. Then a percentage of the poverty line is calculated by subtracting in individual’s income from the poverty line, and dividing by the poverty line. The final index value is found by taking the average of all the calculated percentages.\nThis approach has two main drawbacks, it’s not distribution sensitive and it does not incorporate information about people living above the poverty line. As a solution to the poverty gap index not being distribution sensitive, they squared poverty gap index was created. This approach squares the poverty line percentage so that larger percentages become larger and small percentages aren’t equally weighted. This reintroduces the issue that a squared percentage isn’t an intuitive unit for people to understand when making comparisons.\n\n\n\nThe poverty severity index is an alternative to the squared poverty gap index because the poverty severity index is a sum of the weighted income ratios. This approach begins similarly by setting a poverty line and calculating the ratio of the income to the poverty line. The poverty severity index is then calculated by taking the average of all the squared income ratios.\nThe poverty severity index has the benefit of being distribution sensitive, making it a useful tool for evaluating inequities within a population all living below the poverty line. However, this index also does not include information about people living above the poverty line.\n\n\n\nThe poverty headcount ratio is the most simple index used when looking at poverty within an area of interest, but it also includes the least amount of information about unequaly distributed incomes. The poverty headcount ratio is found by dividing the size of the population living in poverty by the size of the total population.\nThis is by far the simplest of the calculations done so far, which makes it easy for the average person to compare two ratios and have an intuitive understanding of the difference in values. This index also have the benefit of being the only poverty index that includes information about the people living above the poverty line. However, the poverty headcount ratio provides no information about how poor the people living in poverty are, or how rich the people living above the poverty line are.\n\n\n\n\nWhile poverty indices have the benefit of being much more intuitive, the indices that measure inequality are more complex. The inequality indices are therefore better at providing more information about the true inequality within a population.\n\n\nThe Lorenz curve is one of the most commonly used data visualizations to describe income inequality within a population. The Lorenz curve shows what percentage of income is owned by each percentage of the population.\nWhen describing the Lorenz Curve, a straight line connecting both ends of the curve represents equality within a population. However, typically Lorenz curves are curved, showing the unequal distribution of income within a population. The lorenz curve has the beenfit of being transparent, with easy-to-interpret axis, however it requires a more in-depth caluclation to create curves for each population. A visualization of the Lorenz curve can be seen below:\n\n\n\nSource: Wikimedia Foundation. (2023, May 6). Lorenz curve. Wikipedia. https://en.wikipedia.org/wiki/Lorenz_curve\n\n\n\n\n\nThe Gini index was derived from the Lorenz curve and is calculated as the ratio between the area under each of the curves shown in the graph above. The sum of areas A and B in the above graph represent total equality as an area. The area in between the line of equality and the Lorenz curve represents the present inequality as an area. The Gini index takes the ratio of area A and the sum of areas A and B to create a ratio of inequality.\n\n\n\nThe Theil index is a measure of inequality that uses entropy as a metric. Since entropy isn’t a very intuitive metric the formula for calculating the Theil index requires calculating the ratio between the individual’s income and the average income to create an income ratio. This ratio is then multiplied by the natural log of itself and averaged for all individuals within a population.\nThe Theil index essentially measures the “entropic distance” from a fully equal economy, however intuitively understanding the Theil index takes extensive study.\n\n\n\n\nGiven the descriptions of various methods for standardizing poverty and inequality, it might be obvious that there is a need for an index that is capable of capturing the complexities of inequity without requiring every individual to be well-versed in mathematics.\nAs researchers have continued to find ways to solve this dilemma, there are two main front runners for a solutions to the balance of prior poverty and inequality indices: The Welfare, Poverty, and Inequality Index, and the Multidimensional Poverty Index.\n\n\nKraay et al., a Development Economic Group at the World Bank, have found a way to create an index that is distribution sensitive while also having intuitive units. This index is calculated by averaging the ratio between a fixed income threshold and an individual’s income. This is slightly different from the other indices discussed in this paper because the threshold income is in the numerator of the income ratio, and the threshold income doesn’t have to be the poverty line. The threshold income can take on a variety of values including a simple linear equation. The Welfare, Poverty, and Inequality Index describes the “the average factor by which individual incomes need to be multiplied to reach the reference level of income” (Kraay et at., 2023). This is a great alternative for economists, social scientists, and policy makers who hope to provide a similar index to what is being common used in literature, but wish to improve the transparency of the inequality within a population.\n\n\n\nThe United Nations Development Programme along with the Oxford Poverty and Human Development Initiative have developed a multidimensional poverty index that allows for factors other than just poverty to be included in the measurement of inequality. The multi-dimensional poverty index looks at three main factors when determining poverty: Access to healthcare, access to education, and the standard of living for each population of interest. These key factors are then broken up into subcategories that can be scored based on a percentage. These percentages are then weighted and summed to output a poverty index.\n\n\n\nSource: OPHI (2018). Global Multidimensional Poverty Index 2018: The Most Detailed Picture to Date of the World’s Poorest People. Report. Oxford Poverty and Human Development Initiative, University of Oxford.\n\n\n\n\n\n\nWhen studying economics, poverty,or inequality, there are many options available to visualize the data, and each of these options provides different benefits and drawbacks. While poverty indices are a great way to gain insight into how much a population is living below the poverty line, the most common options either lack the complexity to be transparent or have non-intuitive units. Inequality indices are much better at capturing the true complexity of inequality within a population, however not all equality indices require equal understanding to interpret. There are also new indices becoming available as the push for transparency in the inequalities existing in the world becomes unavoidable. It is imperative that biases and personal interests are manged when choosing an index for data visualization, and when analyzing the data visualizations of others.\n\n\n\nFuture work for this paper would involve creating a more extensive list of available indices, which would allow for more opportunities of comparison. Another worthwhile addition to this paper would be the inclusion of more indices that estimate inequality and poverty factors, similar to the multidimesional poverty index.\n\n\n\nBureau, US Census. “Theil Index.” Census.Gov, 8 Oct. 2021, www.census.gov/topics/income-poverty/income-inequality/about/metrics/theil-index.html#:~:text=The%20Theil%20index%20is%20a,everyone%20having%20the%20same%20income.\nCowell, Frank A., and Emmanuel Flachaire. “Inequality Measurement and the Rich: Why ... - Wiley Online Library.” Wiley Online Library, The review of income and wealth, 6 Mar. 2023, onlinelibrary.wiley.com/doi/10.1111/roiw.12638.\nKraay, Aart, Christoph Lakner, Berk Özler, Benoit Decerf, Dean Jolliffe, Olivier Sterck, and Nishant Yonzan. “Reproducibility Package for a New Distribution Sensitive Index for Measuring Welfare, Poverty, and Inequality.” Reproducible Research Repository, The World Bank, 20 July 2023, reproducibility.worldbank.org/index.php/catalog/7#project_desc_container1687292771537.\nKraay, Aart, Christoph Lakner, Berk Özler, Benoit Decerf, Dean Jolliffe, Olivier Sterck, and Nishant Yonzan. “Can We Have a Welfare Index That Is Easy to Understand but Also Distribution Sensitive?” World Bank Blogs, Development Impact, 1 June 2023, blogs.worldbank.org/impactevaluations/can-we-have-welfare-index-easy-understand-also-distribution-sensitive.\nPark, Joongyang, et al. “Measuring Income Inequality Based on Unequally Distributed Income - Journal of Economic Interaction and Coordination.” SpringerLink, Journal of Economic Interaction and Coordination, 13 Aug. 2020, link.springer.com/article/10.1007/s11403-020-00295-1#:~:text=Since%20most%20income%20inequality%20indexes,Theil%20index%20are%20not%20computable.\nUNDP (United Nations Development Programme). 2023. 2023 Global Multidimensional Poverty Index (MPI): Unstacking global poverty: Data for high impact action. New York. https://hdr.undp.org/content/2023-global-multidimensional-poverty-index-mpi#/indicies/MPI\nUnited Nations Economic and Social Commission for Western Asia. “Squared Poverty Gap Index.” Squared Poverty Gap Index, 8 Nov. 2015, archive.unescwa.org/squared-poverty-gap-index#:~:text=Squared%20poverty%20gap%20index%2C%20also,falls%20below%20the%20poverty%20line.\nWorld Bank. 2023. “Poverty and Inequality Platform Methodology Handbook.” Edition 2023-09. Available at https://datanalytics.worldbank.org/PIP-Methodology/."
  },
  {
    "objectID": "blog_posts/2023-11-06_practice_blog_post/index.html#introduction",
    "href": "blog_posts/2023-11-06_practice_blog_post/index.html#introduction",
    "title": "A Comparison of Common Indices Used to Measure Poverty and Inequality",
    "section": "",
    "text": "As an emerging environmental data scientist, I am always looking to learn more about data visualizations that allow technical and non-technical groups to communicate effectively. In the pursuit of equality within countries, and globally, it is important to have a standardized way of comparing wealth disparities. The goal of this paper is to summarize some of the most commonly used indices for measuring poverty and inequalities for a non-technical audience while providing an accurate account of the benefits and drawbacks of each measurement. A description of some less commonly used indices that provide an alternative approach and opportunities for future work have been included as well."
  },
  {
    "objectID": "blog_posts/2023-11-06_practice_blog_post/index.html#types-of-poverty-indices",
    "href": "blog_posts/2023-11-06_practice_blog_post/index.html#types-of-poverty-indices",
    "title": "A Comparison of Common Indices Used to Measure Poverty and Inequality",
    "section": "",
    "text": "In many countries, the easiest and most commonly used indices of success and equality use income data as the only metric. This is partly because monetary data, including average household income and GDP per capita, is easy to collect and paints a picture of a country’s success without being fully transparent about the severity of inequalities. The problem with many of these commonly used poverty indices is that they aren’t always distribution sensitive, meaning incomes slightly below the poverty line are weighted equally compared to an income that is grossly below the poverty line. The benefit of using these indices that aren’t the most transparent about the income inequality is that the units on these indices are generally easy for the average person to interpret. The following poverty indices use income and population data to calculate an index that can be compared between smaller and larger groups regardless of currency.\n\n\nThe poverty gap index provides as estimate of how far below the poverty line people within a population fall. This approach begins by defining the poverty line for each group of interest. Then a percentage of the poverty line is calculated by subtracting in individual’s income from the poverty line, and dividing by the poverty line. The final index value is found by taking the average of all the calculated percentages.\nThis approach has two main drawbacks, it’s not distribution sensitive and it does not incorporate information about people living above the poverty line. As a solution to the poverty gap index not being distribution sensitive, they squared poverty gap index was created. This approach squares the poverty line percentage so that larger percentages become larger and small percentages aren’t equally weighted. This reintroduces the issue that a squared percentage isn’t an intuitive unit for people to understand when making comparisons.\n\n\n\nThe poverty severity index is an alternative to the squared poverty gap index because the poverty severity index is a sum of the weighted income ratios. This approach begins similarly by setting a poverty line and calculating the ratio of the income to the poverty line. The poverty severity index is then calculated by taking the average of all the squared income ratios.\nThe poverty severity index has the benefit of being distribution sensitive, making it a useful tool for evaluating inequities within a population all living below the poverty line. However, this index also does not include information about people living above the poverty line.\n\n\n\nThe poverty headcount ratio is the most simple index used when looking at poverty within an area of interest, but it also includes the least amount of information about unequaly distributed incomes. The poverty headcount ratio is found by dividing the size of the population living in poverty by the size of the total population.\nThis is by far the simplest of the calculations done so far, which makes it easy for the average person to compare two ratios and have an intuitive understanding of the difference in values. This index also have the benefit of being the only poverty index that includes information about the people living above the poverty line. However, the poverty headcount ratio provides no information about how poor the people living in poverty are, or how rich the people living above the poverty line are."
  },
  {
    "objectID": "blog_posts/2023-11-06_practice_blog_post/index.html#types-of-inequality-indices",
    "href": "blog_posts/2023-11-06_practice_blog_post/index.html#types-of-inequality-indices",
    "title": "A Comparison of Common Indices Used to Measure Poverty and Inequality",
    "section": "",
    "text": "While poverty indices have the benefit of being much more intuitive, the indices that measure inequality are more complex. The inequality indices are therefore better at providing more information about the true inequality within a population.\n\n\nThe Lorenz curve is one of the most commonly used data visualizations to describe income inequality within a population. The Lorenz curve shows what percentage of income is owned by each percentage of the population.\nWhen describing the Lorenz Curve, a straight line connecting both ends of the curve represents equality within a population. However, typically Lorenz curves are curved, showing the unequal distribution of income within a population. The lorenz curve has the beenfit of being transparent, with easy-to-interpret axis, however it requires a more in-depth caluclation to create curves for each population. A visualization of the Lorenz curve can be seen below:\n\n\n\nSource: Wikimedia Foundation. (2023, May 6). Lorenz curve. Wikipedia. https://en.wikipedia.org/wiki/Lorenz_curve\n\n\n\n\n\nThe Gini index was derived from the Lorenz curve and is calculated as the ratio between the area under each of the curves shown in the graph above. The sum of areas A and B in the above graph represent total equality as an area. The area in between the line of equality and the Lorenz curve represents the present inequality as an area. The Gini index takes the ratio of area A and the sum of areas A and B to create a ratio of inequality.\n\n\n\nThe Theil index is a measure of inequality that uses entropy as a metric. Since entropy isn’t a very intuitive metric the formula for calculating the Theil index requires calculating the ratio between the individual’s income and the average income to create an income ratio. This ratio is then multiplied by the natural log of itself and averaged for all individuals within a population.\nThe Theil index essentially measures the “entropic distance” from a fully equal economy, however intuitively understanding the Theil index takes extensive study."
  },
  {
    "objectID": "blog_posts/2023-11-06_practice_blog_post/index.html#modern-welfare-indices",
    "href": "blog_posts/2023-11-06_practice_blog_post/index.html#modern-welfare-indices",
    "title": "A Comparison of Common Indices Used to Measure Poverty and Inequality",
    "section": "",
    "text": "Given the descriptions of various methods for standardizing poverty and inequality, it might be obvious that there is a need for an index that is capable of capturing the complexities of inequity without requiring every individual to be well-versed in mathematics.\nAs researchers have continued to find ways to solve this dilemma, there are two main front runners for a solutions to the balance of prior poverty and inequality indices: The Welfare, Poverty, and Inequality Index, and the Multidimensional Poverty Index.\n\n\nKraay et al., a Development Economic Group at the World Bank, have found a way to create an index that is distribution sensitive while also having intuitive units. This index is calculated by averaging the ratio between a fixed income threshold and an individual’s income. This is slightly different from the other indices discussed in this paper because the threshold income is in the numerator of the income ratio, and the threshold income doesn’t have to be the poverty line. The threshold income can take on a variety of values including a simple linear equation. The Welfare, Poverty, and Inequality Index describes the “the average factor by which individual incomes need to be multiplied to reach the reference level of income” (Kraay et at., 2023). This is a great alternative for economists, social scientists, and policy makers who hope to provide a similar index to what is being common used in literature, but wish to improve the transparency of the inequality within a population.\n\n\n\nThe United Nations Development Programme along with the Oxford Poverty and Human Development Initiative have developed a multidimensional poverty index that allows for factors other than just poverty to be included in the measurement of inequality. The multi-dimensional poverty index looks at three main factors when determining poverty: Access to healthcare, access to education, and the standard of living for each population of interest. These key factors are then broken up into subcategories that can be scored based on a percentage. These percentages are then weighted and summed to output a poverty index.\n\n\n\nSource: OPHI (2018). Global Multidimensional Poverty Index 2018: The Most Detailed Picture to Date of the World’s Poorest People. Report. Oxford Poverty and Human Development Initiative, University of Oxford."
  },
  {
    "objectID": "blog_posts/2023-11-06_practice_blog_post/index.html#conclusion",
    "href": "blog_posts/2023-11-06_practice_blog_post/index.html#conclusion",
    "title": "A Comparison of Common Indices Used to Measure Poverty and Inequality",
    "section": "",
    "text": "When studying economics, poverty,or inequality, there are many options available to visualize the data, and each of these options provides different benefits and drawbacks. While poverty indices are a great way to gain insight into how much a population is living below the poverty line, the most common options either lack the complexity to be transparent or have non-intuitive units. Inequality indices are much better at capturing the true complexity of inequality within a population, however not all equality indices require equal understanding to interpret. There are also new indices becoming available as the push for transparency in the inequalities existing in the world becomes unavoidable. It is imperative that biases and personal interests are manged when choosing an index for data visualization, and when analyzing the data visualizations of others."
  },
  {
    "objectID": "blog_posts/2023-11-06_practice_blog_post/index.html#future-work",
    "href": "blog_posts/2023-11-06_practice_blog_post/index.html#future-work",
    "title": "A Comparison of Common Indices Used to Measure Poverty and Inequality",
    "section": "",
    "text": "Future work for this paper would involve creating a more extensive list of available indices, which would allow for more opportunities of comparison. Another worthwhile addition to this paper would be the inclusion of more indices that estimate inequality and poverty factors, similar to the multidimesional poverty index."
  },
  {
    "objectID": "blog_posts/2023-11-06_practice_blog_post/index.html#citations",
    "href": "blog_posts/2023-11-06_practice_blog_post/index.html#citations",
    "title": "A Comparison of Common Indices Used to Measure Poverty and Inequality",
    "section": "",
    "text": "Bureau, US Census. “Theil Index.” Census.Gov, 8 Oct. 2021, www.census.gov/topics/income-poverty/income-inequality/about/metrics/theil-index.html#:~:text=The%20Theil%20index%20is%20a,everyone%20having%20the%20same%20income.\nCowell, Frank A., and Emmanuel Flachaire. “Inequality Measurement and the Rich: Why ... - Wiley Online Library.” Wiley Online Library, The review of income and wealth, 6 Mar. 2023, onlinelibrary.wiley.com/doi/10.1111/roiw.12638.\nKraay, Aart, Christoph Lakner, Berk Özler, Benoit Decerf, Dean Jolliffe, Olivier Sterck, and Nishant Yonzan. “Reproducibility Package for a New Distribution Sensitive Index for Measuring Welfare, Poverty, and Inequality.” Reproducible Research Repository, The World Bank, 20 July 2023, reproducibility.worldbank.org/index.php/catalog/7#project_desc_container1687292771537.\nKraay, Aart, Christoph Lakner, Berk Özler, Benoit Decerf, Dean Jolliffe, Olivier Sterck, and Nishant Yonzan. “Can We Have a Welfare Index That Is Easy to Understand but Also Distribution Sensitive?” World Bank Blogs, Development Impact, 1 June 2023, blogs.worldbank.org/impactevaluations/can-we-have-welfare-index-easy-understand-also-distribution-sensitive.\nPark, Joongyang, et al. “Measuring Income Inequality Based on Unequally Distributed Income - Journal of Economic Interaction and Coordination.” SpringerLink, Journal of Economic Interaction and Coordination, 13 Aug. 2020, link.springer.com/article/10.1007/s11403-020-00295-1#:~:text=Since%20most%20income%20inequality%20indexes,Theil%20index%20are%20not%20computable.\nUNDP (United Nations Development Programme). 2023. 2023 Global Multidimensional Poverty Index (MPI): Unstacking global poverty: Data for high impact action. New York. https://hdr.undp.org/content/2023-global-multidimensional-poverty-index-mpi#/indicies/MPI\nUnited Nations Economic and Social Commission for Western Asia. “Squared Poverty Gap Index.” Squared Poverty Gap Index, 8 Nov. 2015, archive.unescwa.org/squared-poverty-gap-index#:~:text=Squared%20poverty%20gap%20index%2C%20also,falls%20below%20the%20poverty%20line.\nWorld Bank. 2023. “Poverty and Inequality Platform Methodology Handbook.” Edition 2023-09. Available at https://datanalytics.worldbank.org/PIP-Methodology/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome in!",
    "section": "",
    "text": "I’m Heather, and I’m thrilled to share snippets of my life with you here. Showcasing my education in Engineering and Data Science alongside my extracurricular pursuits and passion projects under a single label, or one-page resume, can be challenging. My hope is that this website will provide a holistic view of who I am and serve as a time capsule of my accomplishments in early adulthood. Take a look around and reach out if any of my projects interest you!"
  },
  {
    "objectID": "PasProj.html",
    "href": "PasProj.html",
    "title": "Passion Projects",
    "section": "",
    "text": "Here are some of the passion projects I’ve been working on over the last few years, hopefully you can learn more about what I do outside of school and work :)"
  },
  {
    "objectID": "PasProj.html#yoga",
    "href": "PasProj.html#yoga",
    "title": "Passion Projects",
    "section": "Yoga",
    "text": "Yoga\n[Insert blurb about my yoga journey]"
  },
  {
    "objectID": "PasProj.html#baking",
    "href": "PasProj.html#baking",
    "title": "Passion Projects",
    "section": "Baking",
    "text": "Baking\n[Insert blurb about baking journey w/ links to some of my favorite recipes & mentioning how I baked goodies for each person in the cohort to promote community]"
  },
  {
    "objectID": "PasProj.html#arts-crafts",
    "href": "PasProj.html#arts-crafts",
    "title": "Passion Projects",
    "section": "Arts & Crafts",
    "text": "Arts & Crafts\n[Insert blurb about crafting and talk about how we put on craft nights]\nSkills I’ve Learned:\n\nCricut\n3D painting\nCrochet"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html",
    "href": "Geospatial_Blogs/2023-12-15/index.html",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "",
    "text": "Estimate the number of homes in Houston that lost power as a result of the first two storms during the Houston Blackouts. We will also investigate if socioeconomic factors are predictors of communities recovery from a power outage.\n\n\n\nIn February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\n\n\n\n\n\nUse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\n\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area.  Typically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\n\ngis_osm_roads_free_1.gpkg OpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area.\n\n\n\n\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\ngis_osm_buildings_a_free_1.gpkg\ngis_osm_buildings_a_free_1.gpkg\n\n\n\n\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\nYou can use st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata.\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use. The geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use.\n\n\n\n\nFor step by step walk-through on the initial data cleaning and wrangling can be found on my GitHub repository.\nThere are 3 steps to this analysis:\n\n\nThis analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\n\n\n\nTo determine the number of homes that lost power, we will spatially join these areas with OpenStreetMap data on buildings and roads. To determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\n\n\n\nTo investigate potential socioeconomic factors that influenced recovery, we will link this analysis with data from the US Census Bureau. To investigate potential socioeconomic factors that influenced recovery, we will link this analysis with data from the US Census Bureau. This data can be visualized from the following maps and plot.\n\n#Create a map of median income by census tract, designating which tracts had blackouts\ntm_shape(census_houston) + \n  tm_polygons(\"median_income\", \n          title = \"Median Income ($)\",\n          palette = \"magma\") +\n  tm_title(text = \"Map of Median Income with Blackouts Designated by Centroids\")+\n  tm_shape(trac_with_blackout) +\n  tm_dots() \n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n  tm_shape(trac_with_blackout) +\n  tm_polygons(\"median_income\", \n          title = \"Median Income ($)\",\n          palette = \"RdPu\") +\n    tm_title(text = 'Median Income of Census Tracts that Experienced a Blackout')+\n      tm_basemap(server = \"OpenStreetMap\")\n\n\n\n\n\n  tm_shape(trac_without_blackout) +\n  tm_polygons(\"median_income\", \n          title = \"Median Income ($)\",\n          palette = \"RdPu\") +\n    tm_title(text = 'Median Income of Census Tracts that did not Experience a Blackout')+\n      tm_basemap(server = \"OpenStreetMap\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"RdPu\" is named\n\"brewer.rd_pu\"\nMultiple palettes called \"rd_pu\" found: \"brewer.rd_pu\", \"matplotlib.rd_pu\". The first one, \"brewer.rd_pu\", is returned.\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\nggplot(combined, aes(x=VNP46A1.A2021038.h08v05.001.2021039064328.tif, y=median_income))+\n  geom_boxplot()+\n  labs(x = \"Experienced a Blackout\",\n       y = 'Median Income ($)',\n       title = \"Median Income spearated by Blackout Status\")\n\n\n\n\n\n\n\n\nThis distribution was originally surprising to me because I expected there to be some information about lower-income census tracts being affected by the blackouts. I noticed from this data that so many buildings were affected by the blackouts; they weren’t based on income. The income from the census tracts that didn’t experience a blackout was lower than the census tracts that did because so few census tracts avoided the blackouts. The main limitation that I could identify in this study is that the original data wasn’t available in the same projection. In order to join the spatial data, we had to reproject the data, which required the use of the st_make_valid() function in a few places in order for the code to run. Ideally, we could find data that was available in the same projection, so the geometries were more consistent.\n\n\n\nOne of the most notable thigns about this blackout is that Houston is generally faily well prepared for Hot weather, but this proved their energy systems aren’t resilient to extreme cold weather. I would be interested to see if power outages in Houston due to hot weather have a stronger socioeconomic component. I would also want to replicate this analysis for other cities during natural disasters.\n\n\n\nERCOT Blackout 2021 | Energy Institute. (n.d.). The University of Texas at Austin. https://energy.utexas.edu/research/ercot-blackout-2021\nGeofabrik download server. (n.d.). https://download.geofabrik.de/\nLevel-1 and Atmosphere Archive & Distribution System Distributed Active Archive Center - LAADS DAAC. (2017, February 22). https://ladsweb.modaps.eosdis.nasa.gov/\nPlanet OSM. (n.d.). https://planet.openstreetmap.org/\nPractical Engineering. (2021, March 23). What really happened during the Texas power grid outage? [Video]. YouTube. https://www.youtube.com/watch?v=08mwXICY4JM\nUS Census Bureau. (2023, November 30). American Community Survey (ACS). Census.gov. https://www.census.gov/programs-surveys/acs\nVox. (2021, March 4). Texas’s power disaster is a warning sign for the US [Video]. YouTube. https://www.youtube.com/watch?v=Zcrsgdl_hP0\nWikipedia contributors. (2023, August 12). Visible Infrared Imaging Radiometer Suite. Wikipedia. https://en.wikipedia.org/wiki/Visible_Infrared_Imaging_Radiometer_Suite"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html#problem-statement",
    "href": "Geospatial_Blogs/2023-12-15/index.html#problem-statement",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "",
    "text": "Estimate the number of homes in Houston that lost power as a result of the first two storms during the Houston Blackouts. We will also investigate if socioeconomic factors are predictors of communities recovery from a power outage."
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html#background",
    "href": "Geospatial_Blogs/2023-12-15/index.html#background",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "",
    "text": "In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives."
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html#data-sources",
    "href": "Geospatial_Blogs/2023-12-15/index.html#data-sources",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "",
    "text": "Use NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\n\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area.  Typically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\n\ngis_osm_roads_free_1.gpkg OpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area.\n\n\n\n\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\ngis_osm_buildings_a_free_1.gpkg\ngis_osm_buildings_a_free_1.gpkg\n\n\n\n\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\nYou can use st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata.\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use. The geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html#analysis-and-results",
    "href": "Geospatial_Blogs/2023-12-15/index.html#analysis-and-results",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "",
    "text": "For step by step walk-through on the initial data cleaning and wrangling can be found on my GitHub repository.\nThere are 3 steps to this analysis:\n\n\nThis analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\n\n\n\nTo determine the number of homes that lost power, we will spatially join these areas with OpenStreetMap data on buildings and roads. To determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\n\n\n\nTo investigate potential socioeconomic factors that influenced recovery, we will link this analysis with data from the US Census Bureau. To investigate potential socioeconomic factors that influenced recovery, we will link this analysis with data from the US Census Bureau. This data can be visualized from the following maps and plot.\n\n#Create a map of median income by census tract, designating which tracts had blackouts\ntm_shape(census_houston) + \n  tm_polygons(\"median_income\", \n          title = \"Median Income ($)\",\n          palette = \"magma\") +\n  tm_title(text = \"Map of Median Income with Blackouts Designated by Centroids\")+\n  tm_shape(trac_with_blackout) +\n  tm_dots() \n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n  tm_shape(trac_with_blackout) +\n  tm_polygons(\"median_income\", \n          title = \"Median Income ($)\",\n          palette = \"RdPu\") +\n    tm_title(text = 'Median Income of Census Tracts that Experienced a Blackout')+\n      tm_basemap(server = \"OpenStreetMap\")\n\n\n\n\n\n  tm_shape(trac_without_blackout) +\n  tm_polygons(\"median_income\", \n          title = \"Median Income ($)\",\n          palette = \"RdPu\") +\n    tm_title(text = 'Median Income of Census Tracts that did not Experience a Blackout')+\n      tm_basemap(server = \"OpenStreetMap\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"RdPu\" is named\n\"brewer.rd_pu\"\nMultiple palettes called \"rd_pu\" found: \"brewer.rd_pu\", \"matplotlib.rd_pu\". The first one, \"brewer.rd_pu\", is returned.\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\nggplot(combined, aes(x=VNP46A1.A2021038.h08v05.001.2021039064328.tif, y=median_income))+\n  geom_boxplot()+\n  labs(x = \"Experienced a Blackout\",\n       y = 'Median Income ($)',\n       title = \"Median Income spearated by Blackout Status\")"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html#conclusions",
    "href": "Geospatial_Blogs/2023-12-15/index.html#conclusions",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "",
    "text": "This distribution was originally surprising to me because I expected there to be some information about lower-income census tracts being affected by the blackouts. I noticed from this data that so many buildings were affected by the blackouts; they weren’t based on income. The income from the census tracts that didn’t experience a blackout was lower than the census tracts that did because so few census tracts avoided the blackouts. The main limitation that I could identify in this study is that the original data wasn’t available in the same projection. In order to join the spatial data, we had to reproject the data, which required the use of the st_make_valid() function in a few places in order for the code to run. Ideally, we could find data that was available in the same projection, so the geometries were more consistent."
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html#future-work",
    "href": "Geospatial_Blogs/2023-12-15/index.html#future-work",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "",
    "text": "One of the most notable thigns about this blackout is that Houston is generally faily well prepared for Hot weather, but this proved their energy systems aren’t resilient to extreme cold weather. I would be interested to see if power outages in Houston due to hot weather have a stronger socioeconomic component. I would also want to replicate this analysis for other cities during natural disasters."
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html#citations",
    "href": "Geospatial_Blogs/2023-12-15/index.html#citations",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "",
    "text": "ERCOT Blackout 2021 | Energy Institute. (n.d.). The University of Texas at Austin. https://energy.utexas.edu/research/ercot-blackout-2021\nGeofabrik download server. (n.d.). https://download.geofabrik.de/\nLevel-1 and Atmosphere Archive & Distribution System Distributed Active Archive Center - LAADS DAAC. (2017, February 22). https://ladsweb.modaps.eosdis.nasa.gov/\nPlanet OSM. (n.d.). https://planet.openstreetmap.org/\nPractical Engineering. (2021, March 23). What really happened during the Texas power grid outage? [Video]. YouTube. https://www.youtube.com/watch?v=08mwXICY4JM\nUS Census Bureau. (2023, November 30). American Community Survey (ACS). Census.gov. https://www.census.gov/programs-surveys/acs\nVox. (2021, March 4). Texas’s power disaster is a warning sign for the US [Video]. YouTube. https://www.youtube.com/watch?v=Zcrsgdl_hP0\nWikipedia contributors. (2023, August 12). Visible Infrared Imaging Radiometer Suite. Wikipedia. https://en.wikipedia.org/wiki/Visible_Infrared_Imaging_Radiometer_Suite"
  },
  {
    "objectID": "Geospatial_Blogs/2023-12-15/index.html#footnotes",
    "href": "Geospatial_Blogs/2023-12-15/index.html#footnotes",
    "title": "Exploring the Socioeconomic Impacts of the 2021 Blackout in Houston, Texas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis. - gis_osm_roads_free_1.gpkg↩︎"
  }
]